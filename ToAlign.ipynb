{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ToAlign.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhilreddybilla28/acadacs/blob/main/ToAlign.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEJt302S8FR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c70e148-12b2-427b-9412-1f959bad5e42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UDA'...\n",
            "remote: Enumerating objects: 369, done.\u001b[K\n",
            "remote: Counting objects: 100% (369/369), done.\u001b[K\n",
            "remote: Compressing objects: 100% (263/263), done.\u001b[K\n",
            "remote: Total 369 (delta 127), reused 304 (delta 88), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (369/369), 4.81 MiB | 8.83 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/microsoft/UDA.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/SCDA/src\")\n",
        "sys.path.append(\"/content/SCDA/model/\")\n",
        "sys.path.append(\"/content/SCDA/src/UDA\")"
      ],
      "metadata": {
        "id": "e5aozs972OyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd UDA\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPJMkGCcmSNy",
        "outputId": "34d3a902-ef7f-4ddd-c1d1-279214d112fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UDA\n",
            "assets\t\t    dataset_map  main.py    requirements.txt  trainer\n",
            "CODE_OF_CONDUCT.md  datasets\t models     SECURITY.md       utils\n",
            "configs\t\t    LICENSE\t README.md  SUPPORT.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install timm"
      ],
      "metadata": {
        "id": "zjGcafuH1lh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb4f6fd1-7b18-4e11-8a65-5dc75a39540a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.6.5-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.13.0+cu113)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2022.6.15)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.6.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/OfficeHomeDataset_10072016.zip' -d '/content/data'"
      ],
      "metadata": {
        "id": "_RjRS2dH-ZOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from datasets.transforms import transform_train, transform_test\n",
        "\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "\n",
        "class CommonDataset(Dataset):\n",
        "    def __init__(self, is_train: bool = True):\n",
        "        self.data = []\n",
        "        self.domain_id = []\n",
        "        self.image_root = ''\n",
        "        self.transform = transform_train() if is_train else transform_test()\n",
        "        self._domains = None\n",
        "        self.num_domain = 1\n",
        "\n",
        "    @property\n",
        "    def domains(self):\n",
        "        return self._domains\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # domain = random.randint(0, self.num_domain - 1)\n",
        "        # path, label = self.data[domain][index]\n",
        "        domain = self.domain_id[index]\n",
        "        path, label = self.data[index]\n",
        "        path = os.path.join(self.image_root, path)\n",
        "        with Image.open(path) as image:\n",
        "            image = image.convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'label': label,\n",
        "            'domain': domain\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "6tBXLEgvuhKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "from datasets.common_dataset import CommonDataset\n",
        "from datasets.reader import read_images_labels\n",
        "\n",
        "\n",
        "class OfficeHome(CommonDataset):\n",
        "    \"\"\"\n",
        "    -data_root:\n",
        "     |\n",
        "     |-art\n",
        "     |-clipart\n",
        "     |-product\n",
        "     |-real_world\n",
        "       |-Alarm_Clock\n",
        "         |-0001.jpg\n",
        "    \"\"\"\n",
        "    def __init__(self, data_root, domains: list, status: str = 'train', trim: int = 0):\n",
        "        super().__init__(is_train=(status == 'train'))\n",
        "\n",
        "        self._domains = ['product', 'art', 'clipart', 'real_world']\n",
        "\n",
        "        if domains[0] not in self._domains:\n",
        "            raise ValueError(f'Expected \\'domain\\' in {self._domains}, but got {domains[0]}')\n",
        "        _status = ['train', 'val', 'test']\n",
        "        if status not in _status:\n",
        "            raise ValueError(f'Expected \\'status\\' in {_status}, but got {status}')\n",
        "\n",
        "        self.image_root = data_root\n",
        "\n",
        "        # read txt files\n",
        "        data = read_images_labels(\n",
        "            os.path.join(f'dataset_map/office_home', f'{domains[0]}.txt'),\n",
        "            shuffle=(status == 'train'),\n",
        "            trim=0\n",
        "        )\n",
        "\n",
        "        self.data = data\n",
        "        self.domain_id = [0] * len(self.data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "iSh5xfTzuhX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MODEL"
      ],
      "metadata": {
        "id": "z0E59L8p6uUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "from utils.torch_funcs import init_weights_fc, init_weights_fc0, init_weights_fc1, init_weights_fc2\n",
        "\n",
        "__all__ = ['BaseModel']\n",
        "\n",
        "\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_classes: int = 1000,\n",
        "                 hda: bool = False,  # whether use hda head\n",
        "                 toalign: bool = False,  # whether use toalign\n",
        "                 **kwargs\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self._fdim = None\n",
        "\n",
        "        # HDA\n",
        "        self.hda = hda\n",
        "\n",
        "        # toalign\n",
        "        self.toalign = toalign\n",
        "\n",
        "    def build_head(self):\n",
        "        # classification head\n",
        "        self.fc = nn.Linear(self.fdim, self.num_classes)\n",
        "        nn.init.kaiming_normal_(self.fc.weight)\n",
        "        if self.fc.bias is not None:\n",
        "            nn.init.zeros_(self.fc.bias)\n",
        "\n",
        "        # HDA head\n",
        "        if self.hda:\n",
        "            self.fc.apply(init_weights_fc)\n",
        "            self.fc0 = nn.Linear(self._fdim, self.num_classes)\n",
        "            self.fc0.apply(init_weights_fc0)\n",
        "            self.fc1 = nn.Linear(self._fdim, self.num_classes)\n",
        "            self.fc1.apply(init_weights_fc1)\n",
        "            self.fc2 = nn.Linear(self._fdim, self.num_classes)\n",
        "            self.fc2.apply(init_weights_fc2)\n",
        "\n",
        "    @property\n",
        "    def fdim(self) -> int:\n",
        "        return self._fdim\n",
        "\n",
        "    def get_backbone_parameters(self):\n",
        "        return []\n",
        "\n",
        "    def get_parameters(self):\n",
        "        parameter_list = self.get_backbone_parameters()\n",
        "        parameter_list.append({'params': self.fc.parameters(), 'lr_mult': 10})\n",
        "        if self.hda:\n",
        "            parameter_list.append({'params': self.fc0.parameters(), 'lr_mult': 10})\n",
        "            parameter_list.append({'params': self.fc1.parameters(), 'lr_mult': 10})\n",
        "            parameter_list.append({'params': self.fc2.parameters(), 'lr_mult': 10})\n",
        "\n",
        "        return parameter_list\n",
        "\n",
        "    def forward_backbone(self, x):\n",
        "        \"\"\" input x --> output feature \"\"\"\n",
        "        return x\n",
        "\n",
        "    def _get_toalign_weight(self, f, labels=None):\n",
        "        assert labels is not None, f'labels should be asigned'\n",
        "        w = self.fc.weight[labels].detach()  # [B, C]\n",
        "        if self.hda:\n",
        "            w0 = self.fc0.weight[labels].detach()\n",
        "            w1 = self.fc1.weight[labels].detach()\n",
        "            w2 = self.fc2.weight[labels].detach()\n",
        "            w = w - (w0 + w1 + w2)\n",
        "        eng_org = (f**2).sum(dim=1, keepdim=True)  # [B, 1]\n",
        "        eng_aft = ((f*w)**2).sum(dim=1, keepdim=True)  # [B, 1]\n",
        "        scalar = (eng_org / eng_aft).sqrt()\n",
        "        w_pos = w * scalar\n",
        "\n",
        "        return w_pos\n",
        "\n",
        "    def forward(self, x, toalign=False, labels=None) -> tuple:\n",
        "        \"\"\"\n",
        "        return: [f, y, ...]\n",
        "        \"\"\"\n",
        "        f = self.forward_backbone(x)  # output feature [B, C]\n",
        "        assert f.dim() == 2, f'Expected dim of returned features to be 2, but found {f.dim()}'\n",
        "\n",
        "        if toalign:\n",
        "            w_pos = self._get_toalign_weight(f, labels=labels)\n",
        "            f_pos = f * w_pos\n",
        "            y_pos = self.fc(f_pos)\n",
        "            if self.hda:\n",
        "                z_pos0 = self.fc0(f_pos)\n",
        "                z_pos1 = self.fc1(f_pos)\n",
        "                z_pos2 = self.fc2(f_pos)\n",
        "                z_pos = z_pos0 + z_pos1 + z_pos2\n",
        "                return f_pos, y_pos - z_pos, z_pos\n",
        "            else:\n",
        "                return f_pos, y_pos\n",
        "        else:\n",
        "            y = self.fc(f)\n",
        "            if self.hda:\n",
        "                z0 = self.fc0(f)\n",
        "                z1 = self.fc1(f)\n",
        "                z2 = self.fc2(f)\n",
        "                z = z0 + z1 + z2\n",
        "                return f, y - z, z\n",
        "            else:\n",
        "                return f, y"
      ],
      "metadata": {
        "id": "ak3gwjc_6akr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "from utils.torch_funcs import grl_hook\n",
        "\n",
        "__all__ = ['Discriminator']\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_feature: int, hidden_size: int, out_feature: int = 1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ad_layer1 = nn.Linear(in_feature, hidden_size)\n",
        "        self.ad_layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.ad_layer3 = nn.Linear(hidden_size, out_feature)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "\n",
        "        self._init_params()\n",
        "\n",
        "    def forward(self, x, coeff: float):\n",
        "        x = x * 1.  # to avoid affect the grad from another pipeline to x_0\n",
        "        x.register_hook(grl_hook(coeff))\n",
        "        x = self.ad_layer1.forward(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.ad_layer2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.dropout2(x)\n",
        "        y = self.ad_layer3(x)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def _init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.normal_(m.weight, 1., 0.02)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return [{'params': self.parameters(), 'lr_mult': 10}]"
      ],
      "metadata": {
        "id": "VuX52BOpCTbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from typing import Any, Type, Union, List, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from models.base_model import BaseModel\n",
        "from utils.utils import init_from_pretrained_weights\n",
        "\n",
        "__all__ = ['resnet18', 'resnet34', 'resnet50', 'resnet101']\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-f37072fd.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-b627a593.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-0676ba61.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-63fe2227.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(self, inplanes: int, planes: int, stride: int = 1, downsample: Optional[nn.Module] = None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion: int = 4\n",
        "\n",
        "    def __init__(self, inplanes: int, planes: int, stride: int = 1, downsample: Optional[nn.Module] = None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = conv1x1(inplanes, planes)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv2 = conv3x3(planes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(BaseModel):\n",
        "    def __init__(self,\n",
        "                 block: Type[Union[BasicBlock, Bottleneck]],\n",
        "                 layers: List[int],\n",
        "                 num_classes: int = 1000,\n",
        "                 **kwargs\n",
        "                 ):\n",
        "        super().__init__(num_classes=num_classes, **kwargs)\n",
        "\n",
        "        self.inplanes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.feature_layers = [self.conv1, self.bn1, self.layer1, self.layer2, self.layer3, self.layer4]\n",
        "\n",
        "        self._fdim = 512 * block.expansion\n",
        "\n",
        "        self._init_params()\n",
        "\n",
        "        # head\n",
        "        self.build_head()\n",
        "\n",
        "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int, stride: int = 1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride=stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.normal_(m.weight, 1., 0.02)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def get_backbone_parameters(self):\n",
        "        feature_layers_params = []\n",
        "        for m in self.feature_layers:\n",
        "            feature_layers_params += list(m.parameters())\n",
        "        parameter_list = [{'params': feature_layers_params, 'lr_mult': 1}]\n",
        "\n",
        "        return parameter_list\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        return x\n",
        "\n",
        "    def forward_backbone(self, x):\n",
        "        x = self._forward_impl(x)\n",
        "        x = self.global_avgpool(x)\n",
        "        f = torch.flatten(x, 1)\n",
        "        return f\n",
        "\n",
        "\n",
        "def resnet18(pretrained: bool = True, num_classes: int = 1000, **kwargs: Any) -> ResNet:\n",
        "    model = ResNet(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=num_classes, **kwargs)\n",
        "    if pretrained:\n",
        "        init_from_pretrained_weights(\n",
        "            model,\n",
        "            torch.hub.load_state_dict_from_url(url=model_urls['resnet18'], map_location='cpu', model_dir='downloads')\n",
        "        )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34(pretrained: bool = True, num_classes: int = 1000, **kwargs: Any) -> ResNet:\n",
        "    model = ResNet(block=BasicBlock, layers=[3, 4, 6, 3], num_classes=num_classes, **kwargs)\n",
        "    if pretrained:\n",
        "        init_from_pretrained_weights(\n",
        "            model,\n",
        "            torch.hub.load_state_dict_from_url(url=model_urls['resnet34'], map_location='cpu', model_dir='downloads')\n",
        "        )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(pretrained: bool = True, num_classes: int = 1000, **kwargs: Any) -> ResNet:\n",
        "    model = ResNet(block=Bottleneck, layers=[3, 4, 6, 3], num_classes=num_classes, **kwargs)\n",
        "    if pretrained:\n",
        "        init_from_pretrained_weights(\n",
        "            model,\n",
        "            torch.hub.load_state_dict_from_url(url=model_urls['resnet50'], map_location='cpu', model_dir='downloads')\n",
        "        )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet101(pretrained: bool = True, num_classes: int = 1000, **kwargs: Any) -> ResNet:\n",
        "    model = ResNet(block=Bottleneck, layers=[3, 4, 23, 3], num_classes=num_classes, **kwargs)\n",
        "    if pretrained:\n",
        "        init_from_pretrained_weights(\n",
        "            model,\n",
        "            torch.hub.load_state_dict_from_url(url=model_urls['resnet101'], map_location='cpu', model_dir='downloads')\n",
        "        )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "YlRvkoPnEa0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINER"
      ],
      "metadata": {
        "id": "gx5kztA2M1Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from timm.utils import accuracy, AverageMeter\n",
        "\n",
        "from utils.utils import save_model, write_log\n",
        "from utils.lr_scheduler import inv_lr_scheduler\n",
        "from datasets import *\n",
        "from models import *\n",
        "\n",
        "\n",
        "class BaseTrainer:\n",
        "    def __init__(self, cfg):\n",
        "        self.cfg = cfg\n",
        "\n",
        "        logging.info(f'--> trainer: {self.__class__.__name__}')\n",
        "\n",
        "        self.setup()\n",
        "        self.build_datasets()\n",
        "        self.build_models()\n",
        "        self.resume_from_ckpt()\n",
        "\n",
        "    def setup(self):\n",
        "        self.start_ite = 0\n",
        "        self.ite = 0\n",
        "        self.best_acc = 0.\n",
        "        self.tb_writer = SummaryWriter(self.cfg.TRAIN.OUTPUT_TB)\n",
        "\n",
        "    def build_datasets(self):\n",
        "        logging.info(f'--> building dataset from: {self.cfg.DATASET.NAME}')\n",
        "        self.dataset_loaders = {}\n",
        "\n",
        "        # dataset loaders\n",
        "        if self.cfg.DATASET.NAME == 'office_home':\n",
        "            dataset = OfficeHome\n",
        "        elif self.cfg.DATASET.NAME == 'domainnet':\n",
        "            dataset = DomainNet\n",
        "        else:\n",
        "            raise ValueError(f'Dataset {self.cfg.DATASET.NAME} not found')\n",
        "\n",
        "        self.dataset_loaders['source_train'] = DataLoader(\n",
        "            dataset(self.cfg.DATASET.ROOT, self.cfg.DATASET.SOURCE, status='train'),\n",
        "            batch_size=self.cfg.TRAIN.BATCH_SIZE_SOURCE,\n",
        "            shuffle=True,\n",
        "            num_workers=self.cfg.WORKERS,\n",
        "            drop_last=True\n",
        "        )\n",
        "        self.dataset_loaders['source_test'] = DataLoader(\n",
        "            dataset(self.cfg.DATASET.ROOT, self.cfg.DATASET.SOURCE, status='val', trim=self.cfg.DATASET.TRIM),\n",
        "            batch_size=self.cfg.TRAIN.BATCH_SIZE_TEST,\n",
        "            shuffle=False,\n",
        "            num_workers=self.cfg.WORKERS,\n",
        "            drop_last=False\n",
        "        )\n",
        "        self.dataset_loaders['target_train'] = DataLoader(\n",
        "            dataset(self.cfg.DATASET.ROOT, self.cfg.DATASET.TARGET, status='train'),\n",
        "            batch_size=self.cfg.TRAIN.BATCH_SIZE_TARGET,\n",
        "            shuffle=True,\n",
        "            num_workers=self.cfg.WORKERS,\n",
        "            drop_last=True\n",
        "        )\n",
        "        self.dataset_loaders['target_test'] = DataLoader(\n",
        "            dataset(self.cfg.DATASET.ROOT, self.cfg.DATASET.TARGET, status='test'),\n",
        "            batch_size=self.cfg.TRAIN.BATCH_SIZE_TEST,\n",
        "            shuffle=False,\n",
        "            num_workers=self.cfg.WORKERS,\n",
        "            drop_last=False\n",
        "        )\n",
        "        self.len_src = len(self.dataset_loaders['source_train'])\n",
        "        self.len_tar = len(self.dataset_loaders['target_train'])\n",
        "        logging.info(f'    source {self.cfg.DATASET.SOURCE}: {self.len_src}'\n",
        "                     f'/{len(self.dataset_loaders[\"source_test\"])}')\n",
        "        logging.info(f'    target {self.cfg.DATASET.TARGET}: {self.len_tar}'\n",
        "                     f'/{len(self.dataset_loaders[\"target_test\"])}')\n",
        "\n",
        "    def build_models(self):\n",
        "        logging.info(f'--> building models: {self.cfg.MODEL.BASENET}')\n",
        "        self.base_net = self.build_base_models()\n",
        "        self.registed_models = {'base_net': self.base_net}\n",
        "        parameter_list = self.base_net.get_parameters()\n",
        "        self.model_parameters()\n",
        "        self.build_optim(parameter_list)\n",
        "\n",
        "    def build_base_models(self):\n",
        "        basenet_name = self.cfg.MODEL.BASENET\n",
        "        kwargs = {\n",
        "            'pretrained': self.cfg.MODEL.PRETRAIN,\n",
        "            'num_classes': self.cfg.DATASET.NUM_CLASSES,\n",
        "        }\n",
        "\n",
        "        basenet = eval(basenet_name)(**kwargs).cuda()\n",
        "\n",
        "        return basenet\n",
        "\n",
        "    def model_parameters(self):\n",
        "        for k, v in self.registed_models.items():\n",
        "            logging.info(f'    {k} paras: '\n",
        "                         f'{(sum(p.numel() for p in v.parameters()) / 1e6):.2f}M')\n",
        "\n",
        "    def build_optim(self, parameter_list: list):\n",
        "        self.optimizer = optim.SGD(\n",
        "            parameter_list,\n",
        "            lr=self.cfg.TRAIN.LR,\n",
        "            momentum=self.cfg.OPTIM.MOMENTUM,\n",
        "            weight_decay=self.cfg.OPTIM.WEIGHT_DECAY,\n",
        "            nesterov=True\n",
        "        )\n",
        "        self.lr_scheduler = inv_lr_scheduler\n",
        "\n",
        "    def resume_from_ckpt(self):\n",
        "        last_ckpt = os.path.join(self.cfg.TRAIN.OUTPUT_CKPT, 'models-last.pt')\n",
        "        if os.path.exists(last_ckpt):\n",
        "            ckpt = torch.load(last_ckpt)\n",
        "            for k, v in self.registed_models.items():\n",
        "                v.load_state_dict(ckpt[k])\n",
        "            self.optimizer.load_state_dict(ckpt['optimizer'])\n",
        "            self.start_ite = ckpt['ite']\n",
        "            self.best_acc = ckpt['best_acc']\n",
        "            logging.info(f'> loading ckpt from {last_ckpt} | ite: {self.start_ite} | best_acc: {self.best_acc:.3f}')\n",
        "        else:\n",
        "            logging.info('--> training from scratch')\n",
        "\n",
        "    def train(self):\n",
        "        # start training\n",
        "        for _, v in self.registed_models.items():\n",
        "            v.train()\n",
        "        for self.ite in range(self.start_ite, self.cfg.TRAIN.TTL_ITE):\n",
        "            # test\n",
        "            if self.ite % self.cfg.TRAIN.TEST_FREQ == self.cfg.TRAIN.TEST_FREQ - 1 and self.ite != self.start_ite:\n",
        "                self.base_net.eval()\n",
        "                self.test()\n",
        "                self.base_net.train()\n",
        "\n",
        "            self.current_lr = self.lr_scheduler(\n",
        "                self.optimizer,\n",
        "                ite_rate=self.ite / self.cfg.TRAIN.TTL_ITE * self.cfg.METHOD.HDA.LR_MULT,\n",
        "                lr=self.cfg.TRAIN.LR,\n",
        "            )\n",
        "\n",
        "            # dataloader\n",
        "            if self.ite % self.len_src == 0 or self.ite == self.start_ite:\n",
        "                iter_src = iter(self.dataset_loaders['source_train'])\n",
        "            if self.ite % self.len_tar == 0 or self.ite == self.start_ite:\n",
        "                iter_tar = iter(self.dataset_loaders['target_train'])\n",
        "\n",
        "            # forward one iteration\n",
        "            data_src = iter_src.__next__()\n",
        "            data_tar = iter_tar.__next__()\n",
        "            self.one_step(data_src, data_tar)\n",
        "            if self.ite % self.cfg.TRAIN.SAVE_FREQ == 0 and self.ite != 0:\n",
        "                self.save_model(is_best=False, snap=True)\n",
        "\n",
        "    def one_step(self, data_src, data_tar):\n",
        "        inputs_src, labels_src = data_src['image'].cuda(), data_src['label'].cuda()\n",
        "\n",
        "        outputs_all_src = self.base_net(inputs_src)  # [f, y]\n",
        "\n",
        "        loss_cls_src = F.cross_entropy(outputs_all_src[1], labels_src)\n",
        "\n",
        "        loss_ttl = loss_cls_src\n",
        "\n",
        "        # update\n",
        "        self.step(loss_ttl)\n",
        "\n",
        "        # display\n",
        "        if self.ite % self.cfg.TRAIN.PRINT_FREQ == 0:\n",
        "            self.display([\n",
        "                f'l_cls_src: {loss_cls_src.item():.3f}',\n",
        "                f'l_ttl: {loss_ttl.item():.3f}',\n",
        "                f'best_acc: {self.best_acc:.3f}',\n",
        "            ])\n",
        "            # tensorboard\n",
        "            self.update_tb({\n",
        "                'l_cls_src': loss_cls_src.item(),\n",
        "                'l_ttl': loss_ttl.item(),\n",
        "            })\n",
        "\n",
        "    def display(self, data: list):\n",
        "        log_str = f'I:  {self.ite}/{self.cfg.TRAIN.TTL_ITE} | lr: {self.current_lr:.5f} '\n",
        "        # update\n",
        "        for _str in data:\n",
        "            log_str += '| {} '.format(_str)\n",
        "        logging.info(log_str)\n",
        "\n",
        "    def update_tb(self, data: dict):\n",
        "        for k, v in data.items():\n",
        "            self.tb_writer.add_scalar(k, v, self.ite)\n",
        "\n",
        "    def step(self, loss_ttl):\n",
        "        self.optimizer.zero_grad()\n",
        "        loss_ttl.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def test(self):\n",
        "        logging.info('--> testing on source_test')\n",
        "        src_acc = self.test_func(self.dataset_loaders['source_test'], self.base_net)\n",
        "        logging.info('--> testing on target_test')\n",
        "        tar_acc = self.test_func(self.dataset_loaders['target_test'], self.base_net)\n",
        "        is_best = False\n",
        "        if tar_acc > self.best_acc:\n",
        "            self.best_acc = tar_acc\n",
        "            is_best = True\n",
        "\n",
        "        # display\n",
        "        log_str = f'I:  {self.ite}/{self.cfg.TRAIN.TTL_ITE} | src_acc: {src_acc:.3f} | tar_acc: {tar_acc:.3f} | ' \\\n",
        "                  f'best_acc: {self.best_acc:.3f}'\n",
        "        logging.info(log_str)\n",
        "\n",
        "        # save results\n",
        "        log_dict = {\n",
        "            'I': self.ite,\n",
        "            'src_acc': src_acc,\n",
        "            'tar_acc': tar_acc,\n",
        "            'best_acc': self.best_acc\n",
        "        }\n",
        "        write_log(self.cfg.TRAIN.OUTPUT_RESFILE, log_dict)\n",
        "\n",
        "        # tensorboard\n",
        "        self.tb_writer.add_scalar('tar_acc', tar_acc, self.ite)\n",
        "        self.tb_writer.add_scalar('src_acc', src_acc, self.ite)\n",
        "\n",
        "        self.save_model(is_best=is_best)\n",
        "\n",
        "    def test_func(self, loader, model):\n",
        "        with torch.no_grad():\n",
        "            iter_test = iter(loader)\n",
        "            print_freq = max(len(loader) // 5, self.cfg.TRAIN.PRINT_FREQ)\n",
        "            accs = AverageMeter()\n",
        "            for i in range(len(loader)):\n",
        "                if i % print_freq == print_freq - 1:\n",
        "                    logging.info('    I:  {}/{} | acc: {:.3f}'.format(i, len(loader), accs.avg))\n",
        "                data = iter_test.__next__()\n",
        "                inputs, labels = data['image'].cuda(), data['label'].cuda()\n",
        "                outputs_all = model(inputs)  # [f, y, ...]\n",
        "                outputs = outputs_all[1]\n",
        "\n",
        "                acc = accuracy(outputs, labels)[0]\n",
        "                accs.update(acc.item(), labels.size(0))\n",
        "\n",
        "        return accs.avg\n",
        "\n",
        "    def save_model(self, is_best=False, snap=False):\n",
        "        data_dict = {\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'ite': self.ite,\n",
        "            'best_acc': self.best_acc\n",
        "        }\n",
        "        for k, v in self.registed_models.items():\n",
        "            data_dict.update({k: v.state_dict()})\n",
        "        save_model(self.cfg.TRAIN.OUTPUT_CKPT, data_dict=data_dict, ite=self.ite, is_best=is_best, snap=snap)\n"
      ],
      "metadata": {
        "id": "DAfnzFLDMv8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from utils.torch_funcs import entropy_func\n",
        "from trainer.base_trainer import BaseTrainer\n",
        "from models.discriminator import *\n",
        "from utils.loss import d_align_uda\n",
        "from utils.utils import get_coeff\n",
        "\n",
        "\n",
        "class DANN(BaseTrainer):\n",
        "    def __init__(self, cfg):\n",
        "        super(DANN, self).__init__(cfg)\n",
        "\n",
        "    def build_models(self):\n",
        "        logging.info(f'--> building models: {self.cfg.MODEL.BASENET}')\n",
        "        # backbone\n",
        "        self.base_net = self.build_base_models()\n",
        "        fdim = self.base_net.fdim\n",
        "        # discriminator\n",
        "        self.d_net = eval(self.cfg.MODEL.DNET)(\n",
        "            in_feature=fdim,\n",
        "            hidden_size=self.cfg.MODEL.D_HIDDEN_SIZE,\n",
        "            out_feature=self.cfg.MODEL.D_OUTDIM\n",
        "        ).cuda()\n",
        "\n",
        "        self.registed_models = {'base_net': self.base_net, 'd_net': self.d_net}\n",
        "        self.model_parameters()\n",
        "        parameter_list = self.base_net.get_parameters() + self.d_net.get_parameters()\n",
        "        self.build_optim(parameter_list)\n",
        "\n",
        "    def one_step(self, data_src, data_tar):\n",
        "        inputs_src, labels_src = data_src['image'].cuda(), data_src['label'].cuda()\n",
        "        inputs_tar, labels_tar = data_tar['image'].cuda(), data_tar['label'].cuda()\n",
        "\n",
        "        outputs_all_src = self.base_net(inputs_src)\n",
        "        outputs_all_tar = self.base_net(inputs_tar)\n",
        "\n",
        "        features_all = torch.cat((outputs_all_src[0], outputs_all_tar[0]), dim=0)\n",
        "        logits_all = torch.cat((outputs_all_src[1], outputs_all_tar[1]), dim=0)\n",
        "        softmax_all = nn.Softmax(dim=1)(logits_all)\n",
        "\n",
        "        ent_tar = entropy_func(nn.Softmax(dim=1)(outputs_all_tar[1].data)).mean()\n",
        "\n",
        "        # classificaiton\n",
        "        loss_cls_src = F.cross_entropy(outputs_all_src[1], labels_src)\n",
        "        loss_cls_tar = F.cross_entropy(outputs_all_tar[1].data, labels_tar)\n",
        "\n",
        "        # domain alignment\n",
        "        loss_alg = d_align_uda(\n",
        "            softmax_all, features_all, self.d_net,\n",
        "            coeff=get_coeff(self.ite, max_iter=self.cfg.TRAIN.TTL_ITE), ent=self.cfg.METHOD.ENT\n",
        "        )\n",
        "\n",
        "        loss_ttl = loss_cls_src + loss_alg * self.cfg.METHOD.W_ALG\n",
        "\n",
        "        # update\n",
        "        self.step(loss_ttl)\n",
        "\n",
        "        # display\n",
        "        if self.ite % self.cfg.TRAIN.PRINT_FREQ == 0:\n",
        "            self.display([\n",
        "                f'l_cls_src: {loss_cls_src.item():.3f}',\n",
        "                f'l_cls_tar: {loss_cls_tar.item():.3f}',\n",
        "                f'l_alg: {loss_alg.item():.3f}',\n",
        "                f'l_ttl: {loss_ttl.item():.3f}',\n",
        "                f'ent_tar: {ent_tar.item():.3f}',\n",
        "                f'best_acc: {self.best_acc:.3f}',\n",
        "            ])\n",
        "            # tensorboard\n",
        "            self.update_tb({\n",
        "                'l_cls_src': loss_cls_src.item(),\n",
        "                'l_cls_tar': loss_cls_tar.item(),\n",
        "                'l_alg': loss_alg.item(),\n",
        "                'l_ttl': loss_ttl.item(),\n",
        "                'ent_tar': ent_tar.item(),\n",
        "            })\n"
      ],
      "metadata": {
        "id": "dbeaC8OwUM0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from utils.torch_funcs import entropy_func\n",
        "import utils.loss as loss\n",
        "from trainer.da.dann import DANN\n",
        "from models import *\n",
        "from utils.utils import get_coeff\n",
        "\n",
        "\n",
        "__all__ = ['ToAlign']\n",
        "\n",
        "\n",
        "class ToAlign(DANN):\n",
        "    def __init__(self, cfg):\n",
        "        super(ToAlign, self).__init__(cfg)\n",
        "\n",
        "    def build_base_models(self):\n",
        "        basenet_name = self.cfg.MODEL.BASENET\n",
        "        kwargs = {\n",
        "            'pretrained': self.cfg.MODEL.PRETRAIN,\n",
        "            'num_classes': self.cfg.DATASET.NUM_CLASSES,\n",
        "            'hda': True,\n",
        "            'toalign': True,\n",
        "        }\n",
        "\n",
        "        basenet = eval(basenet_name)(**kwargs).cuda()\n",
        "\n",
        "        return basenet\n",
        "\n",
        "    def build_models(self):\n",
        "        logging.info(f'--> building models: {self.cfg.MODEL.BASENET}')\n",
        "        # backbone\n",
        "        self.base_net = self.build_base_models()\n",
        "        # discriminator\n",
        "        self.d_net = eval(self.cfg.MODEL.DNET)(\n",
        "            in_feature=self.cfg.DATASET.NUM_CLASSES,\n",
        "            hidden_size=self.cfg.MODEL.D_HIDDEN_SIZE,\n",
        "            out_feature=self.cfg.MODEL.D_OUTDIM\n",
        "        ).cuda()\n",
        "\n",
        "        self.registed_models = {'base_net': self.base_net, 'd_net': self.d_net}\n",
        "        self.model_parameters()\n",
        "        parameter_list = self.base_net.get_parameters() + self.d_net.get_parameters()\n",
        "        self.build_optim(parameter_list)\n",
        "\n",
        "    def one_step(self, data_src, data_tar):\n",
        "        inputs_src, labels_src = data_src['image'].cuda(), data_src['label'].cuda()\n",
        "        inputs_tar, labels_tar = data_tar['image'].cuda(), data_tar['label'].cuda()\n",
        "\n",
        "        # --------- classification --------------\n",
        "        outputs_all_src = self.base_net(inputs_src)  # [f, y, z]\n",
        "        assert len(outputs_all_src) == 3, \\\n",
        "            f'Expected return with size 3, but got {len(outputs_all_src)}'\n",
        "        loss_cls_src = F.cross_entropy(outputs_all_src[1], labels_src)\n",
        "        focals_src = outputs_all_src[-1]\n",
        "\n",
        "        # --------- alignment --------------\n",
        "        outputs_all_src = self.base_net(inputs_src, toalign=True, labels=labels_src)  # [f_p, y_p, z_p]\n",
        "        outputs_all_tar = self.base_net(inputs_tar)  # [f, y, z]\n",
        "        assert len(outputs_all_src) == 3 and len(outputs_all_tar) == 3, \\\n",
        "            f'Expected return with size 3, but got {len(outputs_all_src)}'\n",
        "        focals_tar = outputs_all_tar[-1]\n",
        "\n",
        "        logits_all = torch.cat((outputs_all_src[1], outputs_all_tar[1]), dim=0)\n",
        "        softmax_all = nn.Softmax(dim=1)(logits_all)\n",
        "        focals_all = torch.cat((focals_src, focals_tar), dim=0)\n",
        "\n",
        "        ent_tar = entropy_func(nn.Softmax(dim=1)(outputs_all_tar[1].data)).mean()\n",
        "\n",
        "        # classificaiton loss\n",
        "        loss_cls_tar = F.cross_entropy(outputs_all_tar[1].data, labels_tar)\n",
        "\n",
        "        # domain alignment\n",
        "        if self.cfg.TASK.NAME == 'UDA':\n",
        "            loss_alg = loss.d_align_uda(\n",
        "                softmax_output=softmax_all, d_net=self.d_net,\n",
        "                coeff=get_coeff(self.ite, max_iter=self.cfg.TRAIN.TTL_ITE), ent=self.cfg.METHOD.ENT\n",
        "            )\n",
        "        elif self.cfg.TASK.NAME == 'MSDA':\n",
        "            loss_alg = loss.d_align_msda(\n",
        "                softmax_output=softmax_all, d_net=self.d_net,\n",
        "                coeff=get_coeff(self.ite, max_iter=self.cfg.TRAIN.TTL_ITE), ent=self.cfg.METHOD.ENT,\n",
        "                batchsizes=[inputs_src.shape[0], inputs_tar.shape[0]]\n",
        "            )\n",
        "\n",
        "        # hda\n",
        "        loss_hda = focals_all.abs().mean()\n",
        "\n",
        "        loss_ttl = loss_cls_src + loss_alg * self.cfg.METHOD.W_ALG + loss_hda\n",
        "\n",
        "        # update\n",
        "        self.step(loss_ttl)\n",
        "\n",
        "        # display\n",
        "        if self.ite % self.cfg.TRAIN.PRINT_FREQ == 0:\n",
        "            self.display([\n",
        "                f'l_cls_src: {loss_cls_src.item():.3f}',\n",
        "                f'l_cls_tar: {loss_cls_tar.item():.3f}',\n",
        "                f'l_alg: {loss_alg.item():.3f}',\n",
        "                f'l_hda: {loss_hda.item():.3f}',\n",
        "                f'l_ttl: {loss_ttl.item():.3f}',\n",
        "                f'ent_tar: {ent_tar.item():.3f}',\n",
        "                f'best_acc: {self.best_acc:.3f}',\n",
        "            ])\n",
        "            # tensorboard\n",
        "            self.update_tb({\n",
        "                'l_cls_src': loss_cls_src.item(),\n",
        "                'l_cls_tar': loss_cls_tar.item(),\n",
        "                'l_alg': loss_alg.item(),\n",
        "                'l_hda': loss_hda.item(),\n",
        "                'l_ttl': loss_ttl.item(),\n",
        "                'ent_tar': ent_tar.item(),\n",
        "            })\n"
      ],
      "metadata": {
        "id": "CXJXgjVMVfa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN"
      ],
      "metadata": {
        "id": "aiPRiboCePKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "cfg = Namespace()\n",
        "\n",
        "cfg.MODEL = Namespace()\n",
        "cfg.DATASET = Namespace()\n",
        "cfg.TRAIN = Namespace()\n",
        "cfg.METHOD = Namespace()\n",
        "cfg.OPTIM = Namespace()\n",
        "\n",
        "cfg.METHOD.HDA = Namespace()\n",
        "\n",
        "cfg.WORKERS = 5\n",
        "\n",
        "cfg.MODEL.BASENET = \"EmbedNN\"\n",
        "cfg.MODEL.DNET = 'Discriminator'\n",
        "\n",
        "cfg.MODEL.EMBED_INPUT = [3, 131, 4, 483, 103, 5, 106, 4]\n",
        "cfg.MODEL.EMBED_DIM = [1, 3, 1, 4, 3, 1, 3, 1]\n",
        "cfg.MODEL.NUM_DIM = 43\n",
        "\n",
        "cfg.MODEL.D_HIDDEN_SIZE = 32\n",
        "cfg.MODEL.D_OUTDIM = 1\n",
        "\n",
        "cfg.DATASET.NUM_CLASSES = 2\n",
        "\n",
        "cfg.TRAIN.TTL_ITE = 10000\n",
        "cfg.TRAIN.PRINT_FREQ = 16\n",
        "cfg.TRAIN.OUTPUT_CKPT = \"./\"\n",
        "\n",
        "cfg.METHOD.ENT = True\n",
        "cfg.METHOD.W_ALG = 1.0\n",
        "cfg.METHOD.HDA.LR_MULT = 1.0\n",
        "\n",
        "\n",
        "\n",
        "cfg.TRAIN.BATCH_SIZE = 4096\n",
        "\n",
        "cfg.TRAIN.TEST_SIZE = 0.25\n",
        "cfg.TRAIN.TEST_FREQ = 50\n",
        "cfg.TRAIN.CATE_INDEX = 8\n",
        "\n",
        "cfg.OPTIM.MOMENTUM = 0.9\n",
        "cfg.OPTIM.WEIGHT_DECAY = 5e-4"
      ],
      "metadata": {
        "id": "X_ZU1Bh9egtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install yacs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsbnfzHQq2tc",
        "outputId": "bbdaafd6-afbd-4a06-f0d4-77fda0d763ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from yacs) (3.13)\n",
            "Installing collected packages: yacs\n",
            "Successfully installed yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------\n",
        "# Copyright (c) 2021 Microsoft\n",
        "# Licensed under The MIT License\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import os\n",
        "\n",
        "from yacs.config import CfgNode as CN\n",
        "\n",
        "\n",
        "_C = CN()\n",
        "_C.SEED = 123\n",
        "_C.WORKERS = 8\n",
        "_C.TRAINER = 'Trainer'\n",
        "\n",
        "\n",
        "# tasks\n",
        "_C.TASK = CN()\n",
        "_C.TASK.NAME = 'UDA'\n",
        "_C.TASK.SSDA_SHOT = 1\n",
        "\n",
        "# ================= training ====================\n",
        "_C.TRAIN = CN()\n",
        "_C.TRAIN.TEST_FREQ = 500\n",
        "_C.TRAIN.PRINT_FREQ = 50\n",
        "_C.TRAIN.SAVE_FREQ = 5000\n",
        "_C.TRAIN.TTL_ITE = 8000\n",
        "\n",
        "_C.TRAIN.BATCH_SIZE_SOURCE = 36\n",
        "_C.TRAIN.BATCH_SIZE_TARGET = 36\n",
        "_C.TRAIN.BATCH_SIZE_TEST = 36\n",
        "_C.TRAIN.LR = 0.001\n",
        "\n",
        "_C.TRAIN.OUTPUT_ROOT = 'temp'\n",
        "_C.TRAIN.OUTPUT_DIR = ''\n",
        "_C.TRAIN.OUTPUT_LOG = 'log'\n",
        "_C.TRAIN.OUTPUT_TB = 'tensorboard'\n",
        "_C.TRAIN.OUTPUT_CKPT = 'ckpt'\n",
        "_C.TRAIN.OUTPUT_RESFILE = 'log.txt'\n",
        "\n",
        "# ================= models ====================\n",
        "_C.OPTIM = CN()\n",
        "_C.OPTIM.WEIGHT_DECAY = 5e-4\n",
        "_C.OPTIM.MOMENTUM = 0.9\n",
        "\n",
        "# ================= models ====================\n",
        "_C.MODEL = CN()\n",
        "_C.MODEL.PRETRAIN = True\n",
        "_C.MODEL.BASENET = 'resent50'\n",
        "_C.MODEL.BASENET_DOMAIN_EBD = False  # for domain embedding for transformer\n",
        "_C.MODEL.DNET = 'Discriminator'\n",
        "_C.MODEL.D_INDIM = 0\n",
        "_C.MODEL.D_OUTDIM = 1\n",
        "_C.MODEL.D_HIDDEN_SIZE = 1024\n",
        "_C.MODEL.D_WGAN_CLIP = 0.01\n",
        "_C.MODEL.VIT_DPR = 0.1\n",
        "_C.MODEL.VIT_USE_CLS_TOKEN = True\n",
        "_C.MODEL.VIT_PRETRAIN_EXLD = []\n",
        "# extra layer\n",
        "_C.MODEL.EXT_LAYER = False\n",
        "_C.MODEL.EXT_NUM_TOKENS = 100\n",
        "_C.MODEL.EXT_NUM_LAYERS = 1\n",
        "_C.MODEL.EXT_NUM_HEADS = 24\n",
        "_C.MODEL.EXT_LR = 10.\n",
        "_C.MODEL.EXT_DPR = 0.1\n",
        "_C.MODEL.EXT_SKIP = True\n",
        "_C.MODEL.EXT_FEATURE = 768\n",
        "\n",
        "# ================= dataset ====================\n",
        "_C.DATASET = CN()\n",
        "_C.DATASET.ROOT = ''\n",
        "_C.DATASET.NUM_CLASSES = 10\n",
        "_C.DATASET.NAME = 'office_home'\n",
        "_C.DATASET.SOURCE = []\n",
        "_C.DATASET.TARGET = []\n",
        "_C.DATASET.TRIM = 0\n",
        "\n",
        "# ================= method ====================\n",
        "_C.METHOD = CN()\n",
        "_C.METHOD.W_ALG = 1.0\n",
        "_C.METHOD.ENT = False\n",
        "\n",
        "# HDA\n",
        "_C.METHOD.HDA = CN()\n",
        "_C.METHOD.HDA.W_HDA = 1.0\n",
        "_C.METHOD.HDA.LR_MULT = 1.0  # set as 5.0 to tune the lr_schedule to follow the setting of original HDA\n",
        "\n",
        "\n",
        "def get_default_and_update_cfg(args):\n",
        "    cfg = _C.clone()\n",
        "    cfg.merge_from_file(args.cfg)\n",
        "    if args.opts:\n",
        "        cfg.merge_from_list(args.opts)\n",
        "\n",
        "    #\n",
        "    cfg.SEED = args.seed\n",
        "\n",
        "    #\n",
        "    if args.data_root:\n",
        "        cfg.DATASET.ROOT = args.data_root\n",
        "\n",
        "    # dataset maps\n",
        "    maps = {\n",
        "        'office_home': {\n",
        "            'p': 'product',\n",
        "            'a': 'art',\n",
        "            'c': 'clipart',\n",
        "            'r': 'real_world'\n",
        "        },\n",
        "        'domainnet': {\n",
        "            'c': 'clipart',\n",
        "            'i': 'infograph',\n",
        "            'p': 'painting',\n",
        "            'q': 'quickdraw',\n",
        "            'r': 'real',\n",
        "            's': 'sketch',\n",
        "        },\n",
        "    }\n",
        "\n",
        "    # MSDA\n",
        "    if cfg.TASK.NAME == 'MSDA':\n",
        "        args.source = [k for k in maps[cfg.DATASET.NAME].keys()]\n",
        "        args.source.remove(args.target[0])\n",
        "\n",
        "    # source & target\n",
        "    cfg.DATASET.SOURCE = [maps[cfg.DATASET.NAME][_d] if _d in maps[cfg.DATASET.NAME].keys() else _\n",
        "                          for _d in args.source]\n",
        "    cfg.DATASET.TARGET = [maps[cfg.DATASET.NAME][_d] if _d in maps[cfg.DATASET.NAME].keys() else _\n",
        "                          for _d in args.target]\n",
        "\n",
        "    # class\n",
        "    if cfg.DATASET.NAME == 'office_home':\n",
        "        cfg.DATASET.NUM_CLASSES = 65\n",
        "    elif cfg.DATASET.NAME == 'office':\n",
        "        cfg.DATASET.NUM_CLASSES = 31\n",
        "    elif cfg.DATASET.NAME == 'visda-2017':\n",
        "        cfg.DATASET.NUM_CLASSES = 12\n",
        "    elif cfg.DATASET.NAME == 'domainnet' or cfg.DATASET.NAME == 'uda_domainnet':\n",
        "        cfg.DATASET.NUM_CLASSES = 345\n",
        "    elif cfg.DATASET.NAME == 'ssda-domainnet':\n",
        "        cfg.DATASET.NUM_CLASSES = 126\n",
        "    else:\n",
        "        raise NotImplementedError(f'cfg.DATASET.NAME: {cfg.DATASET.NAME} not imeplemented')\n",
        "\n",
        "    # output\n",
        "    if args.output_root:\n",
        "        cfg.TRAIN.OUTPUT_ROOT = args.output_root\n",
        "    if args.output_dir:\n",
        "        cfg.TRAIN.OUTPUT_DIR = args.output_dir\n",
        "    else:\n",
        "        cfg.TRAIN.OUTPUT_DIR = '_'.join(cfg.DATASET.SOURCE) + '2' + '_'.join(cfg.DATASET.TARGET) + '_' + str(args.seed)\n",
        "\n",
        "    #\n",
        "    cfg.TRAIN.OUTPUT_CKPT = os.path.join(cfg.TRAIN.OUTPUT_ROOT, 'ckpt', cfg.TRAIN.OUTPUT_DIR)\n",
        "    cfg.TRAIN.OUTPUT_LOG = os.path.join(cfg.TRAIN.OUTPUT_ROOT, 'log', cfg.TRAIN.OUTPUT_DIR)\n",
        "    cfg.TRAIN.OUTPUT_TB = os.path.join(cfg.TRAIN.OUTPUT_ROOT, 'tensorboard', cfg.TRAIN.OUTPUT_DIR)\n",
        "    os.makedirs(cfg.TRAIN.OUTPUT_CKPT, exist_ok=True)\n",
        "    os.makedirs(cfg.TRAIN.OUTPUT_LOG, exist_ok=True)\n",
        "    os.makedirs(cfg.TRAIN.OUTPUT_TB, exist_ok=True)\n",
        "    cfg.TRAIN.OUTPUT_RESFILE = os.path.join(cfg.TRAIN.OUTPUT_LOG, 'log.txt')\n",
        "\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def check_cfg(cfg):\n",
        "    # OUTPUT\n",
        "    cfg.TRAIN.OUTPUT_CKPT = os.path.join(cfg.TRAIN.OUTPUT_ROOT, 'ckpt', cfg.TRAIN.OUTPUT_DIR)\n",
        "    cfg.TRAIN.OUTPUT_LOG = os.path.join(cfg.TRAIN.OUTPUT_ROOT, 'log', cfg.TRAIN.OUTPUT_DIR)\n",
        "    cfg.TRAIN.OUTPUT_TB = os.path.join(cfg.TRAIN.OUTPUT_ROOT, 'tensorboard', cfg.TRAIN.OUTPUT_DIR)\n",
        "    os.makedirs(cfg.TRAIN.OUTPUT_CKPT, exist_ok=True)\n",
        "    os.makedirs(cfg.TRAIN.OUTPUT_LOG, exist_ok=True)\n",
        "    os.makedirs(cfg.TRAIN.OUTPUT_TB, exist_ok=True)\n",
        "    cfg.TRAIN.OUTPUT_RESFILE = os.path.join(cfg.TRAIN.OUTPUT_LOG, 'log.txt')\n",
        "\n",
        "    # dataset\n",
        "    maps = {\n",
        "        'office_home': {\n",
        "            'p': 'product',\n",
        "            'a': 'art',\n",
        "            'c': 'clipart',\n",
        "            'r': 'real_world'\n",
        "        }\n",
        "    }\n",
        "    cfg.DATASET.SOURCE = [maps[cfg.DATASET.NAME][_d] if _d in maps[cfg.DATASET.NAME].keys() else _\n",
        "                          for _d in cfg.DATASET.SOURCE]\n",
        "    cfg.DATASET.TARGET = [maps[cfg.DATASET.NAME][_d] if _d in maps[cfg.DATASET.NAME].keys() else _\n",
        "                          for _d in cfg.DATASET.TARGET]\n",
        "\n",
        "    datapath_list = {\n",
        "        'office-home': {\n",
        "            'p': ['Product.txt', 'Product.txt'],\n",
        "            'a': ['Art.txt', 'Art.txt'],\n",
        "            'c': ['Clipart.txt', 'Clipart.txt'],\n",
        "            'r': ['Real_World.txt', 'Real_World.txt']\n",
        "        },\n",
        "        'uda_domainnet': {\n",
        "            'c': ['clipart_train.txt', 'clipart_test.txt'],\n",
        "            'i': ['infograph_train.txt', 'infograph_test.txt'],\n",
        "            'p': ['painting_train.txt', 'painting_test.txt'],\n",
        "            'q': ['quickdraw_train.txt', 'quickdraw_test.txt'],\n",
        "            'r': ['real_train.txt', 'real_test.txt'],\n",
        "            's': ['sketch_train.txt', 'sketch_test.txt']\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # class\n",
        "    if cfg.DATASET.NAME == 'office_home':\n",
        "        cfg.DATASET.NUM_CLASSES = 65\n",
        "    elif cfg.DATASET.NAME == 'domainnet' or cfg.DATASET.NAME == 'uda_domainnet':\n",
        "        cfg.DATASET.NUM_CLASSES = 345\n",
        "    else:\n",
        "        raise NotImplementedError(f'cfg.DATASET.NAME: {cfg.DATASET.NAME} not imeplemented')\n",
        "\n",
        "    return cfg\n"
      ],
      "metadata": {
        "id": "BjYRFO9DguUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# source and target domains can be defined by \"--source\" and \"--target\"\n",
        "root = '/content/data/office_home' \n",
        "s = 'a'\n",
        "t = 'r'\n",
        "op = 'exp'\n",
        "! python main.py configs/uda_office_home_toalign.yaml --data_root '/content/data/office_home' --source 'a' --target 'r' --output_root 'exp'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZmWGWqXzIA8",
        "outputId": "8875c974-0a8b-4878-812c-40495db9c2f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[2022-07-25 10:08:21]\u001b[0m ======================= args =======================\n",
            "{\n",
            "    \"cfg\": \"configs/uda_office_home_toalign.yaml\",\n",
            "    \"seed\": 123,\n",
            "    \"source\": [\n",
            "        \"a\"\n",
            "    ],\n",
            "    \"target\": [\n",
            "        \"r\"\n",
            "    ],\n",
            "    \"output_root\": \"exp\",\n",
            "    \"output_dir\": null,\n",
            "    \"data_root\": \"/content/data/office_home\",\n",
            "    \"opts\": null\n",
            "}\n",
            "\u001b[32m[2022-07-25 10:08:21]\u001b[0m ======================= cfg =======================\n",
            "DATASET:\n",
            "    NAME: office_home\n",
            "    NUM_CLASSES: 65\n",
            "    ROOT: /content/data/office_home\n",
            "    SOURCE: [art]\n",
            "    TARGET: [real_world]\n",
            "    TRIM: 0\n",
            "METHOD:\n",
            "    ENT: true\n",
            "    HDA: {LR_MULT: 1.0, W_HDA: 1.0}\n",
            "    W_ALG: 1.0\n",
            "MODEL:\n",
            "    BASENET: resnet50\n",
            "    BASENET_DOMAIN_EBD: false\n",
            "    DNET: Discriminator\n",
            "    D_HIDDEN_SIZE: 1024\n",
            "    D_INDIM: 1024\n",
            "    D_OUTDIM: 1\n",
            "    D_WGAN_CLIP: 0.01\n",
            "    EXT_DPR: 0.1\n",
            "    EXT_FEATURE: 768\n",
            "    EXT_LAYER: false\n",
            "    EXT_LR: 10.0\n",
            "    EXT_NUM_HEADS: 24\n",
            "    EXT_NUM_LAYERS: 1\n",
            "    EXT_NUM_TOKENS: 100\n",
            "    EXT_SKIP: true\n",
            "    PRETRAIN: true\n",
            "    VIT_DPR: 0.1\n",
            "    VIT_PRETRAIN_EXLD: []\n",
            "    VIT_USE_CLS_TOKEN: true\n",
            "OPTIM: {MOMENTUM: 0.9, WEIGHT_DECAY: 0.0005}\n",
            "SEED: 123\n",
            "TASK: {NAME: UDA, SSDA_SHOT: 1}\n",
            "TRAIN: {BATCH_SIZE_SOURCE: 36, BATCH_SIZE_TARGET: 36, BATCH_SIZE_TEST: 36, LR: 0.001,\n",
            "    OUTPUT_CKPT: exp/ckpt/art2real_world_123, OUTPUT_DIR: art2real_world_123, OUTPUT_LOG: exp/log/art2real_world_123,\n",
            "    OUTPUT_RESFILE: exp/log/art2real_world_123/log.txt, OUTPUT_ROOT: exp, OUTPUT_TB: exp/tensorboard/art2real_world_123,\n",
            "    PRINT_FREQ: 50, SAVE_FREQ: 5000, TEST_FREQ: 500, TTL_ITE: 8000}\n",
            "TRAINER: ToAlign\n",
            "WORKERS: 8\n",
            "\n",
            "\u001b[32m[2022-07-25 10:08:21]\u001b[0m --> trainer: ToAlign\n",
            "\u001b[32m[2022-07-25 10:08:23]\u001b[0m NumExpr defaulting to 2 threads.\n",
            "\u001b[32m[2022-07-25 10:08:23]\u001b[0m --> building dataset from: office_home\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\u001b[32m[2022-07-25 10:08:23]\u001b[0m     source ['art']: 67/68\n",
            "\u001b[32m[2022-07-25 10:08:23]\u001b[0m     target ['real_world']: 121/122\n",
            "\u001b[32m[2022-07-25 10:08:23]\u001b[0m --> building models: resnet50\n",
            "\u001b[32m[2022-07-25 10:08:23]\u001b[0m     models keys / loaded keys: 326/265\n",
            "\u001b[32m[2022-07-25 10:08:26]\u001b[0m     base_net paras: 24.04M\n",
            "\u001b[32m[2022-07-25 10:08:26]\u001b[0m     d_net paras: 1.12M\n",
            "\u001b[32m[2022-07-25 10:08:26]\u001b[0m --> training from scratch\n",
            "\u001b[32m[2022-07-25 10:08:56]\u001b[0m I:  0/8000 | lr: 0.00100 | l_cls_src: 4.937 | l_cls_tar: 4.468 | l_alg: 0.696 | l_hda: 0.925 | l_ttl: 6.558 | ent_tar: 3.528 | best_acc: 0.000 \n",
            "\u001b[32m[2022-07-25 10:10:05]\u001b[0m I:  50/8000 | lr: 0.00096 | l_cls_src: 1.212 | l_cls_tar: 1.193 | l_alg: 0.700 | l_hda: 1.044 | l_ttl: 2.957 | ent_tar: 1.238 | best_acc: 0.000 \n",
            "\u001b[32m[2022-07-25 10:11:24]\u001b[0m I:  100/8000 | lr: 0.00092 | l_cls_src: 0.300 | l_cls_tar: 1.459 | l_alg: 0.685 | l_hda: 0.804 | l_ttl: 1.788 | ent_tar: 1.267 | best_acc: 0.000 \n",
            "\u001b[32m[2022-07-25 10:12:47]\u001b[0m I:  150/8000 | lr: 0.00088 | l_cls_src: 0.231 | l_cls_tar: 1.194 | l_alg: 0.636 | l_hda: 0.709 | l_ttl: 1.576 | ent_tar: 1.144 | best_acc: 0.000 \n",
            "\u001b[32m[2022-07-25 10:14:03]\u001b[0m I:  200/8000 | lr: 0.00085 | l_cls_src: 0.209 | l_cls_tar: 0.748 | l_alg: 0.671 | l_hda: 0.625 | l_ttl: 1.504 | ent_tar: 0.944 | best_acc: 0.000 \n",
            "\u001b[32m[2022-07-25 10:15:30]\u001b[0m I:  250/8000 | lr: 0.00082 | l_cls_src: 0.118 | l_cls_tar: 0.510 | l_alg: 0.666 | l_hda: 0.524 | l_ttl: 1.308 | ent_tar: 0.842 | best_acc: 0.000 \n",
            "\u001b[32m[2022-07-25 10:16:53]\u001b[0m I:  300/8000 | lr: 0.00079 | l_cls_src: 0.143 | l_cls_tar: 0.649 | l_alg: 0.693 | l_hda: 0.488 | l_ttl: 1.325 | ent_tar: 0.909 | best_acc: 0.000 \n",
            "\u001b[32m[2022-07-25 10:18:09]\u001b[0m I:  350/8000 | lr: 0.00076 | l_cls_src: 0.142 | l_cls_tar: 0.607 | l_alg: 0.675 | l_hda: 0.431 | l_ttl: 1.248 | ent_tar: 1.032 | best_acc: 0.000 \n",
            "\u001b[32m[2022-07-25 10:19:28]\u001b[0m I:  400/8000 | lr: 0.00074 | l_cls_src: 0.114 | l_cls_tar: 0.898 | l_alg: 0.623 | l_hda: 0.406 | l_ttl: 1.143 | ent_tar: 1.181 | best_acc: 0.000 \n",
            "\u001b[32m[2022-07-25 10:20:46]\u001b[0m I:  450/8000 | lr: 0.00072 | l_cls_src: 0.060 | l_cls_tar: 1.090 | l_alg: 0.641 | l_hda: 0.359 | l_ttl: 1.060 | ent_tar: 0.868 | best_acc: 0.000 \n",
            "\u001b[32m[2022-07-25 10:22:12]\u001b[0m --> testing on source_test\n",
            "\u001b[32m[2022-07-25 10:22:42]\u001b[0m     I:  49/68 | acc: 99.433\n",
            "\u001b[32m[2022-07-25 10:22:47]\u001b[0m --> testing on target_test\n",
            "\u001b[32m[2022-07-25 10:23:47]\u001b[0m     I:  49/122 | acc: 74.490\n",
            "\u001b[32m[2022-07-25 10:24:38]\u001b[0m     I:  99/122 | acc: 77.048\n",
            "\u001b[32m[2022-07-25 10:24:53]\u001b[0m I:  499/8000 | src_acc: 99.547 | tar_acc: 76.842 | best_acc: 76.842\n",
            "\u001b[32m[2022-07-25 10:24:54]\u001b[0m     models saved to exp/ckpt/art2real_world_123/models-last.pt\n",
            "\u001b[32m[2022-07-25 10:24:54]\u001b[0m     keys: dict_keys(['optimizer', 'ite', 'best_acc', 'base_net', 'd_net'])\n",
            "\u001b[32m[2022-07-25 10:24:55]\u001b[0m     best models saved to exp/ckpt/art2real_world_123/models-best.pt\n",
            "\u001b[32m[2022-07-25 10:24:57]\u001b[0m I:  500/8000 | lr: 0.00069 | l_cls_src: 0.063 | l_cls_tar: 0.835 | l_alg: 0.629 | l_hda: 0.341 | l_ttl: 1.034 | ent_tar: 1.054 | best_acc: 76.842 \n",
            "\u001b[32m[2022-07-25 10:26:11]\u001b[0m I:  550/8000 | lr: 0.00068 | l_cls_src: 0.062 | l_cls_tar: 1.448 | l_alg: 0.610 | l_hda: 0.316 | l_ttl: 0.988 | ent_tar: 0.867 | best_acc: 76.842 \n",
            "\u001b[32m[2022-07-25 10:27:25]\u001b[0m I:  600/8000 | lr: 0.00066 | l_cls_src: 0.113 | l_cls_tar: 0.750 | l_alg: 0.618 | l_hda: 0.306 | l_ttl: 1.038 | ent_tar: 0.964 | best_acc: 76.842 \n",
            "\u001b[32m[2022-07-25 10:29:01]\u001b[0m I:  650/8000 | lr: 0.00064 | l_cls_src: 0.074 | l_cls_tar: 0.867 | l_alg: 0.641 | l_hda: 0.290 | l_ttl: 1.005 | ent_tar: 0.872 | best_acc: 76.842 \n",
            "\u001b[32m[2022-07-25 10:30:15]\u001b[0m I:  700/8000 | lr: 0.00062 | l_cls_src: 0.103 | l_cls_tar: 0.937 | l_alg: 0.641 | l_hda: 0.265 | l_ttl: 1.009 | ent_tar: 0.485 | best_acc: 76.842 \n",
            "\u001b[32m[2022-07-25 10:31:42]\u001b[0m I:  750/8000 | lr: 0.00061 | l_cls_src: 0.033 | l_cls_tar: 1.090 | l_alg: 0.628 | l_hda: 0.257 | l_ttl: 0.918 | ent_tar: 1.230 | best_acc: 76.842 \n",
            "\u001b[32m[2022-07-25 10:32:56]\u001b[0m I:  800/8000 | lr: 0.00059 | l_cls_src: 0.056 | l_cls_tar: 0.876 | l_alg: 0.615 | l_hda: 0.247 | l_ttl: 0.918 | ent_tar: 0.611 | best_acc: 76.842 \n",
            "\u001b[32m[2022-07-25 10:34:23]\u001b[0m I:  850/8000 | lr: 0.00058 | l_cls_src: 0.068 | l_cls_tar: 1.526 | l_alg: 0.693 | l_hda: 0.234 | l_ttl: 0.996 | ent_tar: 0.728 | best_acc: 76.842 \n",
            "\u001b[32m[2022-07-25 10:35:43]\u001b[0m I:  900/8000 | lr: 0.00057 | l_cls_src: 0.066 | l_cls_tar: 1.194 | l_alg: 0.633 | l_hda: 0.227 | l_ttl: 0.925 | ent_tar: 0.851 | best_acc: 76.842 \n",
            "\u001b[32m[2022-07-25 10:37:03]\u001b[0m I:  950/8000 | lr: 0.00056 | l_cls_src: 0.064 | l_cls_tar: 1.516 | l_alg: 0.668 | l_hda: 0.218 | l_ttl: 0.949 | ent_tar: 0.650 | best_acc: 76.842 \n",
            "\u001b[32m[2022-07-25 10:38:20]\u001b[0m --> testing on source_test\n",
            "\u001b[32m[2022-07-25 10:38:52]\u001b[0m     I:  49/68 | acc: 99.546\n",
            "\u001b[32m[2022-07-25 10:38:56]\u001b[0m --> testing on target_test\n",
            "\u001b[32m[2022-07-25 10:39:57]\u001b[0m     I:  49/122 | acc: 78.118\n",
            "\u001b[32m[2022-07-25 10:40:47]\u001b[0m     I:  99/122 | acc: 78.423\n",
            "\u001b[32m[2022-07-25 10:41:01]\u001b[0m I:  999/8000 | src_acc: 99.629 | tar_acc: 77.944 | best_acc: 77.944\n",
            "\u001b[32m[2022-07-25 10:41:02]\u001b[0m     models saved to exp/ckpt/art2real_world_123/models-last.pt\n",
            "\u001b[32m[2022-07-25 10:41:02]\u001b[0m     keys: dict_keys(['optimizer', 'ite', 'best_acc', 'base_net', 'd_net'])\n",
            "\u001b[32m[2022-07-25 10:41:03]\u001b[0m     best models saved to exp/ckpt/art2real_world_123/models-best.pt\n",
            "\u001b[32m[2022-07-25 10:41:05]\u001b[0m I:  1000/8000 | lr: 0.00054 | l_cls_src: 0.020 | l_cls_tar: 0.672 | l_alg: 0.670 | l_hda: 0.216 | l_ttl: 0.906 | ent_tar: 0.766 | best_acc: 77.944 \n",
            "\u001b[32m[2022-07-25 10:42:22]\u001b[0m I:  1050/8000 | lr: 0.00053 | l_cls_src: 0.105 | l_cls_tar: 0.891 | l_alg: 0.648 | l_hda: 0.200 | l_ttl: 0.953 | ent_tar: 0.603 | best_acc: 77.944 \n",
            "\u001b[32m[2022-07-25 10:43:42]\u001b[0m I:  1100/8000 | lr: 0.00052 | l_cls_src: 0.045 | l_cls_tar: 1.034 | l_alg: 0.686 | l_hda: 0.197 | l_ttl: 0.928 | ent_tar: 0.583 | best_acc: 77.944 \n",
            "\u001b[32m[2022-07-25 10:45:03]\u001b[0m I:  1150/8000 | lr: 0.00051 | l_cls_src: 0.024 | l_cls_tar: 1.359 | l_alg: 0.645 | l_hda: 0.197 | l_ttl: 0.866 | ent_tar: 0.417 | best_acc: 77.944 \n",
            "\u001b[32m[2022-07-25 10:46:15]\u001b[0m I:  1200/8000 | lr: 0.00050 | l_cls_src: 0.067 | l_cls_tar: 0.640 | l_alg: 0.651 | l_hda: 0.182 | l_ttl: 0.900 | ent_tar: 0.535 | best_acc: 77.944 \n",
            "\u001b[32m[2022-07-25 10:47:42]\u001b[0m I:  1250/8000 | lr: 0.00049 | l_cls_src: 0.021 | l_cls_tar: 0.624 | l_alg: 0.672 | l_hda: 0.178 | l_ttl: 0.871 | ent_tar: 0.433 | best_acc: 77.944 \n",
            "\u001b[32m[2022-07-25 10:49:03]\u001b[0m I:  1300/8000 | lr: 0.00048 | l_cls_src: 0.019 | l_cls_tar: 0.798 | l_alg: 0.672 | l_hda: 0.179 | l_ttl: 0.869 | ent_tar: 0.508 | best_acc: 77.944 \n",
            "\u001b[32m[2022-07-25 10:50:27]\u001b[0m I:  1350/8000 | lr: 0.00048 | l_cls_src: 0.021 | l_cls_tar: 0.808 | l_alg: 0.627 | l_hda: 0.172 | l_ttl: 0.820 | ent_tar: 0.403 | best_acc: 77.944 \n",
            "\u001b[32m[2022-07-25 10:51:41]\u001b[0m I:  1400/8000 | lr: 0.00047 | l_cls_src: 0.047 | l_cls_tar: 0.805 | l_alg: 0.672 | l_hda: 0.169 | l_ttl: 0.888 | ent_tar: 0.556 | best_acc: 77.944 \n",
            "\u001b[32m[2022-07-25 10:52:56]\u001b[0m I:  1450/8000 | lr: 0.00046 | l_cls_src: 0.029 | l_cls_tar: 0.695 | l_alg: 0.632 | l_hda: 0.164 | l_ttl: 0.825 | ent_tar: 0.603 | best_acc: 77.944 \n",
            "\u001b[32m[2022-07-25 10:54:18]\u001b[0m --> testing on source_test\n",
            "\u001b[32m[2022-07-25 10:54:48]\u001b[0m     I:  49/68 | acc: 99.546\n",
            "\u001b[32m[2022-07-25 10:54:52]\u001b[0m --> testing on target_test\n",
            "\u001b[32m[2022-07-25 10:55:52]\u001b[0m     I:  49/122 | acc: 80.272\n",
            "\u001b[32m[2022-07-25 10:56:42]\u001b[0m     I:  99/122 | acc: 80.612\n",
            "\u001b[32m[2022-07-25 10:56:57]\u001b[0m I:  1499/8000 | src_acc: 99.629 | tar_acc: 79.803 | best_acc: 79.803\n",
            "\u001b[32m[2022-07-25 10:56:57]\u001b[0m     models saved to exp/ckpt/art2real_world_123/models-last.pt\n",
            "\u001b[32m[2022-07-25 10:56:57]\u001b[0m     keys: dict_keys(['optimizer', 'ite', 'best_acc', 'base_net', 'd_net'])\n",
            "\u001b[32m[2022-07-25 10:56:58]\u001b[0m     best models saved to exp/ckpt/art2real_world_123/models-best.pt\n",
            "\u001b[32m[2022-07-25 10:57:01]\u001b[0m I:  1500/8000 | lr: 0.00045 | l_cls_src: 0.056 | l_cls_tar: 1.055 | l_alg: 0.635 | l_hda: 0.161 | l_ttl: 0.852 | ent_tar: 0.394 | best_acc: 79.803 \n",
            "\u001b[32m[2022-07-25 10:58:16]\u001b[0m I:  1550/8000 | lr: 0.00045 | l_cls_src: 0.024 | l_cls_tar: 0.733 | l_alg: 0.670 | l_hda: 0.156 | l_ttl: 0.849 | ent_tar: 0.421 | best_acc: 79.803 \n",
            "\u001b[32m[2022-07-25 10:59:37]\u001b[0m I:  1600/8000 | lr: 0.00044 | l_cls_src: 0.012 | l_cls_tar: 0.631 | l_alg: 0.669 | l_hda: 0.153 | l_ttl: 0.834 | ent_tar: 0.497 | best_acc: 79.803 \n",
            "\u001b[32m[2022-07-25 11:00:52]\u001b[0m I:  1650/8000 | lr: 0.00043 | l_cls_src: 0.052 | l_cls_tar: 0.955 | l_alg: 0.707 | l_hda: 0.151 | l_ttl: 0.909 | ent_tar: 0.308 | best_acc: 79.803 \n",
            "\u001b[32m[2022-07-25 11:02:19]\u001b[0m I:  1700/8000 | lr: 0.00043 | l_cls_src: 0.011 | l_cls_tar: 0.710 | l_alg: 0.639 | l_hda: 0.148 | l_ttl: 0.799 | ent_tar: 0.541 | best_acc: 79.803 \n",
            "\u001b[32m[2022-07-25 11:03:39]\u001b[0m I:  1750/8000 | lr: 0.00042 | l_cls_src: 0.056 | l_cls_tar: 1.084 | l_alg: 0.685 | l_hda: 0.141 | l_ttl: 0.882 | ent_tar: 0.288 | best_acc: 79.803 \n",
            "\u001b[32m[2022-07-25 11:04:51]\u001b[0m I:  1800/8000 | lr: 0.00041 | l_cls_src: 0.034 | l_cls_tar: 1.177 | l_alg: 0.682 | l_hda: 0.148 | l_ttl: 0.864 | ent_tar: 0.345 | best_acc: 79.803 \n",
            "\u001b[32m[2022-07-25 11:06:16]\u001b[0m I:  1850/8000 | lr: 0.00041 | l_cls_src: 0.034 | l_cls_tar: 0.617 | l_alg: 0.657 | l_hda: 0.137 | l_ttl: 0.828 | ent_tar: 0.228 | best_acc: 79.803 \n",
            "\u001b[32m[2022-07-25 11:07:34]\u001b[0m I:  1900/8000 | lr: 0.00040 | l_cls_src: 0.067 | l_cls_tar: 1.774 | l_alg: 0.687 | l_hda: 0.137 | l_ttl: 0.891 | ent_tar: 0.218 | best_acc: 79.803 \n",
            "\u001b[32m[2022-07-25 11:09:02]\u001b[0m I:  1950/8000 | lr: 0.00040 | l_cls_src: 0.013 | l_cls_tar: 1.331 | l_alg: 0.656 | l_hda: 0.137 | l_ttl: 0.806 | ent_tar: 0.284 | best_acc: 79.803 \n",
            "\u001b[32m[2022-07-25 11:10:10]\u001b[0m --> testing on source_test\n",
            "\u001b[32m[2022-07-25 11:10:42]\u001b[0m     I:  49/68 | acc: 99.603\n",
            "\u001b[32m[2022-07-25 11:10:46]\u001b[0m --> testing on target_test\n",
            "\u001b[32m[2022-07-25 11:11:47]\u001b[0m     I:  49/122 | acc: 80.329\n",
            "\u001b[32m[2022-07-25 11:12:36]\u001b[0m     I:  99/122 | acc: 80.752\n",
            "\u001b[32m[2022-07-25 11:12:51]\u001b[0m I:  1999/8000 | src_acc: 99.629 | tar_acc: 80.032 | best_acc: 80.032\n",
            "\u001b[32m[2022-07-25 11:12:52]\u001b[0m     models saved to exp/ckpt/art2real_world_123/models-last.pt\n",
            "\u001b[32m[2022-07-25 11:12:52]\u001b[0m     keys: dict_keys(['optimizer', 'ite', 'best_acc', 'base_net', 'd_net'])\n",
            "\u001b[32m[2022-07-25 11:12:52]\u001b[0m     best models saved to exp/ckpt/art2real_world_123/models-best.pt\n",
            "\u001b[32m[2022-07-25 11:12:55]\u001b[0m I:  2000/8000 | lr: 0.00039 | l_cls_src: 0.030 | l_cls_tar: 1.624 | l_alg: 0.661 | l_hda: 0.133 | l_ttl: 0.823 | ent_tar: 0.539 | best_acc: 80.032 \n",
            "\u001b[32m[2022-07-25 11:14:10]\u001b[0m I:  2050/8000 | lr: 0.00039 | l_cls_src: 0.019 | l_cls_tar: 1.105 | l_alg: 0.658 | l_hda: 0.132 | l_ttl: 0.809 | ent_tar: 0.392 | best_acc: 80.032 \n",
            "\u001b[32m[2022-07-25 11:15:35]\u001b[0m I:  2100/8000 | lr: 0.00038 | l_cls_src: 0.016 | l_cls_tar: 1.001 | l_alg: 0.675 | l_hda: 0.135 | l_ttl: 0.825 | ent_tar: 0.312 | best_acc: 80.032 \n",
            "\u001b[32m[2022-07-25 11:16:56]\u001b[0m I:  2150/8000 | lr: 0.00038 | l_cls_src: 0.007 | l_cls_tar: 0.455 | l_alg: 0.648 | l_hda: 0.133 | l_ttl: 0.788 | ent_tar: 0.450 | best_acc: 80.032 \n",
            "\u001b[32m[2022-07-25 11:18:16]\u001b[0m I:  2200/8000 | lr: 0.00037 | l_cls_src: 0.013 | l_cls_tar: 0.624 | l_alg: 0.681 | l_hda: 0.129 | l_ttl: 0.823 | ent_tar: 0.220 | best_acc: 80.032 \n",
            "\u001b[32m[2022-07-25 11:19:35]\u001b[0m I:  2250/8000 | lr: 0.00037 | l_cls_src: 0.012 | l_cls_tar: 0.554 | l_alg: 0.675 | l_hda: 0.124 | l_ttl: 0.810 | ent_tar: 0.132 | best_acc: 80.032 \n",
            "\u001b[32m[2022-07-25 11:20:58]\u001b[0m I:  2300/8000 | lr: 0.00036 | l_cls_src: 0.020 | l_cls_tar: 1.567 | l_alg: 0.685 | l_hda: 0.124 | l_ttl: 0.829 | ent_tar: 0.297 | best_acc: 80.032 \n",
            "\u001b[32m[2022-07-25 11:22:18]\u001b[0m I:  2350/8000 | lr: 0.00036 | l_cls_src: 0.024 | l_cls_tar: 0.931 | l_alg: 0.707 | l_hda: 0.119 | l_ttl: 0.850 | ent_tar: 0.386 | best_acc: 80.032 \n",
            "\u001b[32m[2022-07-25 11:23:31]\u001b[0m I:  2400/8000 | lr: 0.00035 | l_cls_src: 0.057 | l_cls_tar: 0.702 | l_alg: 0.664 | l_hda: 0.120 | l_ttl: 0.841 | ent_tar: 0.229 | best_acc: 80.032 \n",
            "\u001b[32m[2022-07-25 11:25:00]\u001b[0m I:  2450/8000 | lr: 0.00035 | l_cls_src: 0.019 | l_cls_tar: 0.359 | l_alg: 0.668 | l_hda: 0.121 | l_ttl: 0.808 | ent_tar: 0.197 | best_acc: 80.032 \n",
            "\u001b[32m[2022-07-25 11:26:15]\u001b[0m --> testing on source_test\n",
            "\u001b[32m[2022-07-25 11:26:45]\u001b[0m     I:  49/68 | acc: 99.603\n",
            "\u001b[32m[2022-07-25 11:26:49]\u001b[0m --> testing on target_test\n",
            "\u001b[32m[2022-07-25 11:27:50]\u001b[0m     I:  49/122 | acc: 80.726\n",
            "\u001b[32m[2022-07-25 11:28:40]\u001b[0m     I:  99/122 | acc: 81.061\n",
            "\u001b[32m[2022-07-25 11:28:55]\u001b[0m I:  2499/8000 | src_acc: 99.629 | tar_acc: 80.239 | best_acc: 80.239\n",
            "\u001b[32m[2022-07-25 11:28:55]\u001b[0m     models saved to exp/ckpt/art2real_world_123/models-last.pt\n",
            "\u001b[32m[2022-07-25 11:28:55]\u001b[0m     keys: dict_keys(['optimizer', 'ite', 'best_acc', 'base_net', 'd_net'])\n",
            "\u001b[32m[2022-07-25 11:28:56]\u001b[0m     best models saved to exp/ckpt/art2real_world_123/models-best.pt\n",
            "\u001b[32m[2022-07-25 11:28:58]\u001b[0m I:  2500/8000 | lr: 0.00035 | l_cls_src: 0.016 | l_cls_tar: 0.196 | l_alg: 0.653 | l_hda: 0.120 | l_ttl: 0.789 | ent_tar: 0.167 | best_acc: 80.239 \n",
            "\u001b[32m[2022-07-25 11:30:22]\u001b[0m I:  2550/8000 | lr: 0.00034 | l_cls_src: 0.009 | l_cls_tar: 0.326 | l_alg: 0.641 | l_hda: 0.115 | l_ttl: 0.765 | ent_tar: 0.192 | best_acc: 80.239 \n",
            "\u001b[32m[2022-07-25 11:31:34]\u001b[0m I:  2600/8000 | lr: 0.00034 | l_cls_src: 0.014 | l_cls_tar: 1.051 | l_alg: 0.650 | l_hda: 0.114 | l_ttl: 0.778 | ent_tar: 0.440 | best_acc: 80.239 \n",
            "\u001b[32m[2022-07-25 11:32:52]\u001b[0m I:  2650/8000 | lr: 0.00033 | l_cls_src: 0.121 | l_cls_tar: 0.811 | l_alg: 0.646 | l_hda: 0.112 | l_ttl: 0.879 | ent_tar: 0.303 | best_acc: 80.239 \n",
            "\u001b[32m[2022-07-25 11:34:16]\u001b[0m I:  2700/8000 | lr: 0.00033 | l_cls_src: 0.046 | l_cls_tar: 0.866 | l_alg: 0.647 | l_hda: 0.110 | l_ttl: 0.804 | ent_tar: 0.214 | best_acc: 80.239 \n",
            "\u001b[32m[2022-07-25 11:35:34]\u001b[0m I:  2750/8000 | lr: 0.00033 | l_cls_src: 0.006 | l_cls_tar: 1.042 | l_alg: 0.656 | l_hda: 0.109 | l_ttl: 0.771 | ent_tar: 0.273 | best_acc: 80.239 \n",
            "\u001b[32m[2022-07-25 11:36:56]\u001b[0m I:  2800/8000 | lr: 0.00032 | l_cls_src: 0.049 | l_cls_tar: 1.871 | l_alg: 0.666 | l_hda: 0.113 | l_ttl: 0.828 | ent_tar: 0.482 | best_acc: 80.239 \n",
            "\u001b[32m[2022-07-25 11:38:13]\u001b[0m I:  2850/8000 | lr: 0.00032 | l_cls_src: 0.010 | l_cls_tar: 1.117 | l_alg: 0.619 | l_hda: 0.113 | l_ttl: 0.742 | ent_tar: 0.390 | best_acc: 80.239 \n",
            "\u001b[32m[2022-07-25 11:39:31]\u001b[0m I:  2900/8000 | lr: 0.00032 | l_cls_src: 0.007 | l_cls_tar: 0.557 | l_alg: 0.641 | l_hda: 0.108 | l_ttl: 0.756 | ent_tar: 0.334 | best_acc: 80.239 \n",
            "\u001b[32m[2022-07-25 11:41:02]\u001b[0m I:  2950/8000 | lr: 0.00031 | l_cls_src: 0.007 | l_cls_tar: 0.830 | l_alg: 0.706 | l_hda: 0.106 | l_ttl: 0.818 | ent_tar: 0.223 | best_acc: 80.239 \n",
            "\u001b[32m[2022-07-25 11:42:10]\u001b[0m --> testing on source_test\n",
            "\u001b[32m[2022-07-25 11:42:43]\u001b[0m     I:  49/68 | acc: 99.603\n",
            "\u001b[32m[2022-07-25 11:42:48]\u001b[0m --> testing on target_test\n",
            "\u001b[32m[2022-07-25 11:43:48]\u001b[0m     I:  49/122 | acc: 81.066\n",
            "\u001b[32m[2022-07-25 11:44:38]\u001b[0m     I:  99/122 | acc: 81.089\n",
            "\u001b[32m[2022-07-25 11:44:53]\u001b[0m I:  2999/8000 | src_acc: 99.629 | tar_acc: 80.376 | best_acc: 80.376\n",
            "\u001b[32m[2022-07-25 11:44:53]\u001b[0m     models saved to exp/ckpt/art2real_world_123/models-last.pt\n",
            "\u001b[32m[2022-07-25 11:44:53]\u001b[0m     keys: dict_keys(['optimizer', 'ite', 'best_acc', 'base_net', 'd_net'])\n",
            "\u001b[32m[2022-07-25 11:44:54]\u001b[0m     best models saved to exp/ckpt/art2real_world_123/models-best.pt\n",
            "\u001b[32m[2022-07-25 11:44:57]\u001b[0m I:  3000/8000 | lr: 0.00031 | l_cls_src: 0.013 | l_cls_tar: 1.141 | l_alg: 0.673 | l_hda: 0.103 | l_ttl: 0.789 | ent_tar: 0.376 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 11:46:20]\u001b[0m I:  3050/8000 | lr: 0.00031 | l_cls_src: 0.012 | l_cls_tar: 1.625 | l_alg: 0.656 | l_hda: 0.106 | l_ttl: 0.773 | ent_tar: 0.280 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 11:47:38]\u001b[0m I:  3100/8000 | lr: 0.00030 | l_cls_src: 0.003 | l_cls_tar: 1.447 | l_alg: 0.658 | l_hda: 0.103 | l_ttl: 0.764 | ent_tar: 0.212 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 11:49:12]\u001b[0m I:  3150/8000 | lr: 0.00030 | l_cls_src: 0.013 | l_cls_tar: 0.880 | l_alg: 0.682 | l_hda: 0.103 | l_ttl: 0.797 | ent_tar: 0.076 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 11:50:26]\u001b[0m I:  3200/8000 | lr: 0.00030 | l_cls_src: 0.023 | l_cls_tar: 0.853 | l_alg: 0.636 | l_hda: 0.100 | l_ttl: 0.759 | ent_tar: 0.224 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 11:51:41]\u001b[0m I:  3250/8000 | lr: 0.00030 | l_cls_src: 0.157 | l_cls_tar: 1.597 | l_alg: 0.661 | l_hda: 0.101 | l_ttl: 0.919 | ent_tar: 0.300 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 11:53:07]\u001b[0m I:  3300/8000 | lr: 0.00029 | l_cls_src: 0.023 | l_cls_tar: 0.719 | l_alg: 0.677 | l_hda: 0.104 | l_ttl: 0.803 | ent_tar: 0.259 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 11:54:26]\u001b[0m I:  3350/8000 | lr: 0.00029 | l_cls_src: 0.007 | l_cls_tar: 0.629 | l_alg: 0.701 | l_hda: 0.096 | l_ttl: 0.804 | ent_tar: 0.231 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 11:55:44]\u001b[0m I:  3400/8000 | lr: 0.00029 | l_cls_src: 0.005 | l_cls_tar: 1.397 | l_alg: 0.627 | l_hda: 0.096 | l_ttl: 0.728 | ent_tar: 0.291 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 11:57:05]\u001b[0m I:  3450/8000 | lr: 0.00029 | l_cls_src: 0.009 | l_cls_tar: 1.007 | l_alg: 0.675 | l_hda: 0.095 | l_ttl: 0.779 | ent_tar: 0.180 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 11:58:20]\u001b[0m --> testing on source_test\n",
            "\u001b[32m[2022-07-25 11:58:45]\u001b[0m     I:  49/68 | acc: 99.603\n",
            "\u001b[32m[2022-07-25 11:58:50]\u001b[0m --> testing on target_test\n",
            "\u001b[32m[2022-07-25 11:59:51]\u001b[0m     I:  49/122 | acc: 81.236\n",
            "\u001b[32m[2022-07-25 12:00:41]\u001b[0m     I:  99/122 | acc: 81.173\n",
            "\u001b[32m[2022-07-25 12:00:56]\u001b[0m I:  3499/8000 | src_acc: 99.629 | tar_acc: 80.376 | best_acc: 80.376\n",
            "\u001b[32m[2022-07-25 12:00:56]\u001b[0m     models saved to exp/ckpt/art2real_world_123/models-last.pt\n",
            "\u001b[32m[2022-07-25 12:00:56]\u001b[0m     keys: dict_keys(['optimizer', 'ite', 'best_acc', 'base_net', 'd_net'])\n",
            "\u001b[32m[2022-07-25 12:00:57]\u001b[0m     best models saved to exp/ckpt/art2real_world_123/models-best.pt\n",
            "\u001b[32m[2022-07-25 12:00:59]\u001b[0m I:  3500/8000 | lr: 0.00028 | l_cls_src: 0.019 | l_cls_tar: 0.487 | l_alg: 0.704 | l_hda: 0.097 | l_ttl: 0.819 | ent_tar: 0.185 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 12:02:21]\u001b[0m I:  3550/8000 | lr: 0.00028 | l_cls_src: 0.018 | l_cls_tar: 0.840 | l_alg: 0.675 | l_hda: 0.092 | l_ttl: 0.785 | ent_tar: 0.167 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 12:03:38]\u001b[0m I:  3600/8000 | lr: 0.00028 | l_cls_src: 0.054 | l_cls_tar: 1.200 | l_alg: 0.700 | l_hda: 0.094 | l_ttl: 0.848 | ent_tar: 0.104 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 12:05:07]\u001b[0m I:  3650/8000 | lr: 0.00028 | l_cls_src: 0.006 | l_cls_tar: 1.876 | l_alg: 0.661 | l_hda: 0.094 | l_ttl: 0.761 | ent_tar: 0.202 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 12:06:25]\u001b[0m I:  3700/8000 | lr: 0.00027 | l_cls_src: 0.011 | l_cls_tar: 1.150 | l_alg: 0.677 | l_hda: 0.093 | l_ttl: 0.780 | ent_tar: 0.163 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 12:07:40]\u001b[0m I:  3750/8000 | lr: 0.00027 | l_cls_src: 0.009 | l_cls_tar: 1.279 | l_alg: 0.679 | l_hda: 0.092 | l_ttl: 0.780 | ent_tar: 0.177 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 12:09:14]\u001b[0m I:  3800/8000 | lr: 0.00027 | l_cls_src: 0.009 | l_cls_tar: 0.810 | l_alg: 0.682 | l_hda: 0.092 | l_ttl: 0.783 | ent_tar: 0.154 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 12:10:32]\u001b[0m I:  3850/8000 | lr: 0.00027 | l_cls_src: 0.008 | l_cls_tar: 1.103 | l_alg: 0.685 | l_hda: 0.089 | l_ttl: 0.782 | ent_tar: 0.120 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 12:11:58]\u001b[0m I:  3900/8000 | lr: 0.00026 | l_cls_src: 0.014 | l_cls_tar: 0.866 | l_alg: 0.653 | l_hda: 0.089 | l_ttl: 0.757 | ent_tar: 0.202 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 12:13:14]\u001b[0m I:  3950/8000 | lr: 0.00026 | l_cls_src: 0.029 | l_cls_tar: 1.971 | l_alg: 0.656 | l_hda: 0.090 | l_ttl: 0.774 | ent_tar: 0.344 | best_acc: 80.376 \n",
            "\u001b[32m[2022-07-25 12:14:34]\u001b[0m --> testing on source_test\n",
            "\u001b[32m[2022-07-25 12:15:07]\u001b[0m     I:  49/68 | acc: 99.603\n",
            "\u001b[32m[2022-07-25 12:15:12]\u001b[0m --> testing on target_test\n",
            "\u001b[32m[2022-07-25 12:16:13]\u001b[0m     I:  49/122 | acc: 81.122\n",
            "\u001b[32m[2022-07-25 12:17:03]\u001b[0m     I:  99/122 | acc: 81.257\n",
            "\u001b[32m[2022-07-25 12:17:18]\u001b[0m I:  3999/8000 | src_acc: 99.629 | tar_acc: 80.399 | best_acc: 80.399\n",
            "\u001b[32m[2022-07-25 12:17:18]\u001b[0m     models saved to exp/ckpt/art2real_world_123/models-last.pt\n",
            "\u001b[32m[2022-07-25 12:17:18]\u001b[0m     keys: dict_keys(['optimizer', 'ite', 'best_acc', 'base_net', 'd_net'])\n",
            "\u001b[32m[2022-07-25 12:17:19]\u001b[0m     best models saved to exp/ckpt/art2real_world_123/models-best.pt\n",
            "\u001b[32m[2022-07-25 12:17:21]\u001b[0m I:  4000/8000 | lr: 0.00026 | l_cls_src: 0.028 | l_cls_tar: 0.988 | l_alg: 0.676 | l_hda: 0.088 | l_ttl: 0.792 | ent_tar: 0.242 | best_acc: 80.399 \n",
            "\u001b[32m[2022-07-25 12:18:33]\u001b[0m I:  4050/8000 | lr: 0.00026 | l_cls_src: 0.093 | l_cls_tar: 0.378 | l_alg: 0.640 | l_hda: 0.087 | l_ttl: 0.819 | ent_tar: 0.144 | best_acc: 80.399 \n",
            "\u001b[32m[2022-07-25 12:19:53]\u001b[0m I:  4100/8000 | lr: 0.00026 | l_cls_src: 0.006 | l_cls_tar: 1.091 | l_alg: 0.701 | l_hda: 0.089 | l_ttl: 0.796 | ent_tar: 0.135 | best_acc: 80.399 \n",
            "\u001b[32m[2022-07-25 12:21:14]\u001b[0m I:  4150/8000 | lr: 0.00025 | l_cls_src: 0.044 | l_cls_tar: 0.242 | l_alg: 0.649 | l_hda: 0.086 | l_ttl: 0.779 | ent_tar: 0.143 | best_acc: 80.399 \n",
            "\u001b[32m[2022-07-25 12:22:36]\u001b[0m I:  4200/8000 | lr: 0.00025 | l_cls_src: 0.011 | l_cls_tar: 0.735 | l_alg: 0.663 | l_hda: 0.083 | l_ttl: 0.757 | ent_tar: 0.151 | best_acc: 80.399 \n",
            "\u001b[32m[2022-07-25 12:23:57]\u001b[0m I:  4250/8000 | lr: 0.00025 | l_cls_src: 0.009 | l_cls_tar: 1.532 | l_alg: 0.665 | l_hda: 0.086 | l_ttl: 0.760 | ent_tar: 0.253 | best_acc: 80.399 \n",
            "\u001b[32m[2022-07-25 12:25:18]\u001b[0m I:  4300/8000 | lr: 0.00025 | l_cls_src: 0.040 | l_cls_tar: 0.506 | l_alg: 0.669 | l_hda: 0.085 | l_ttl: 0.794 | ent_tar: 0.144 | best_acc: 80.399 \n",
            "\u001b[32m[2022-07-25 12:26:31]\u001b[0m I:  4350/8000 | lr: 0.00025 | l_cls_src: 0.027 | l_cls_tar: 0.854 | l_alg: 0.737 | l_hda: 0.085 | l_ttl: 0.848 | ent_tar: 0.119 | best_acc: 80.399 \n",
            "\u001b[32m[2022-07-25 12:28:02]\u001b[0m I:  4400/8000 | lr: 0.00025 | l_cls_src: 0.007 | l_cls_tar: 0.792 | l_alg: 0.710 | l_hda: 0.083 | l_ttl: 0.800 | ent_tar: 0.097 | best_acc: 80.399 \n",
            "\u001b[32m[2022-07-25 12:29:15]\u001b[0m I:  4450/8000 | lr: 0.00024 | l_cls_src: 0.008 | l_cls_tar: 0.844 | l_alg: 0.654 | l_hda: 0.085 | l_ttl: 0.748 | ent_tar: 0.214 | best_acc: 80.399 \n",
            "\u001b[32m[2022-07-25 12:30:39]\u001b[0m --> testing on source_test\n",
            "\u001b[32m[2022-07-25 12:31:09]\u001b[0m     I:  49/68 | acc: 99.603\n",
            "\u001b[32m[2022-07-25 12:31:13]\u001b[0m --> testing on target_test\n",
            "\u001b[32m[2022-07-25 12:32:14]\u001b[0m     I:  49/122 | acc: 81.576\n",
            "\u001b[32m[2022-07-25 12:33:03]\u001b[0m     I:  99/122 | acc: 81.285\n",
            "\u001b[32m[2022-07-25 12:33:18]\u001b[0m I:  4499/8000 | src_acc: 99.629 | tar_acc: 80.445 | best_acc: 80.445\n",
            "\u001b[32m[2022-07-25 12:33:19]\u001b[0m     models saved to exp/ckpt/art2real_world_123/models-last.pt\n",
            "\u001b[32m[2022-07-25 12:33:19]\u001b[0m     keys: dict_keys(['optimizer', 'ite', 'best_acc', 'base_net', 'd_net'])\n",
            "\u001b[32m[2022-07-25 12:33:19]\u001b[0m     best models saved to exp/ckpt/art2real_world_123/models-best.pt\n",
            "\u001b[32m[2022-07-25 12:33:22]\u001b[0m I:  4500/8000 | lr: 0.00024 | l_cls_src: 0.008 | l_cls_tar: 0.440 | l_alg: 0.685 | l_hda: 0.082 | l_ttl: 0.775 | ent_tar: 0.126 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 12:34:31]\u001b[0m I:  4550/8000 | lr: 0.00024 | l_cls_src: 0.006 | l_cls_tar: 0.656 | l_alg: 0.661 | l_hda: 0.082 | l_ttl: 0.749 | ent_tar: 0.080 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 12:35:56]\u001b[0m I:  4600/8000 | lr: 0.00024 | l_cls_src: 0.037 | l_cls_tar: 0.355 | l_alg: 0.675 | l_hda: 0.077 | l_ttl: 0.788 | ent_tar: 0.205 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 12:37:15]\u001b[0m I:  4650/8000 | lr: 0.00024 | l_cls_src: 0.014 | l_cls_tar: 1.219 | l_alg: 0.733 | l_hda: 0.082 | l_ttl: 0.828 | ent_tar: 0.229 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 12:38:32]\u001b[0m I:  4700/8000 | lr: 0.00024 | l_cls_src: 0.003 | l_cls_tar: 1.130 | l_alg: 0.662 | l_hda: 0.081 | l_ttl: 0.745 | ent_tar: 0.199 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 12:39:53]\u001b[0m I:  4750/8000 | lr: 0.00023 | l_cls_src: 0.010 | l_cls_tar: 1.562 | l_alg: 0.666 | l_hda: 0.080 | l_ttl: 0.756 | ent_tar: 0.278 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 12:41:16]\u001b[0m I:  4800/8000 | lr: 0.00023 | l_cls_src: 0.006 | l_cls_tar: 1.128 | l_alg: 0.645 | l_hda: 0.078 | l_ttl: 0.729 | ent_tar: 0.328 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 12:42:37]\u001b[0m I:  4850/8000 | lr: 0.00023 | l_cls_src: 0.003 | l_cls_tar: 1.010 | l_alg: 0.654 | l_hda: 0.081 | l_ttl: 0.738 | ent_tar: 0.195 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 12:43:58]\u001b[0m I:  4900/8000 | lr: 0.00023 | l_cls_src: 0.033 | l_cls_tar: 0.932 | l_alg: 0.688 | l_hda: 0.077 | l_ttl: 0.798 | ent_tar: 0.184 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 12:45:09]\u001b[0m I:  4950/8000 | lr: 0.00023 | l_cls_src: 0.005 | l_cls_tar: 1.409 | l_alg: 0.656 | l_hda: 0.079 | l_ttl: 0.740 | ent_tar: 0.063 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 12:46:40]\u001b[0m --> testing on source_test\n",
            "\u001b[32m[2022-07-25 12:47:07]\u001b[0m     I:  49/68 | acc: 99.603\n",
            "\u001b[32m[2022-07-25 12:47:11]\u001b[0m --> testing on target_test\n",
            "\u001b[32m[2022-07-25 12:48:11]\u001b[0m     I:  49/122 | acc: 81.349\n",
            "\u001b[32m[2022-07-25 12:49:01]\u001b[0m     I:  99/122 | acc: 81.117\n",
            "\u001b[32m[2022-07-25 12:49:16]\u001b[0m I:  4999/8000 | src_acc: 99.629 | tar_acc: 80.353 | best_acc: 80.445\n",
            "\u001b[32m[2022-07-25 12:49:16]\u001b[0m     models saved to exp/ckpt/art2real_world_123/models-last.pt\n",
            "\u001b[32m[2022-07-25 12:49:16]\u001b[0m     keys: dict_keys(['optimizer', 'ite', 'best_acc', 'base_net', 'd_net'])\n",
            "\u001b[32m[2022-07-25 12:49:19]\u001b[0m I:  5000/8000 | lr: 0.00023 | l_cls_src: 0.005 | l_cls_tar: 0.839 | l_alg: 0.653 | l_hda: 0.074 | l_ttl: 0.732 | ent_tar: 0.166 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 12:49:19]\u001b[0m     models saved to exp/ckpt/art2real_world_123/models-005000.pt\n",
            "\u001b[32m[2022-07-25 12:49:19]\u001b[0m     keys: dict_keys(['optimizer', 'ite', 'best_acc', 'base_net', 'd_net'])\n",
            "\u001b[32m[2022-07-25 12:50:30]\u001b[0m I:  5050/8000 | lr: 0.00022 | l_cls_src: 0.010 | l_cls_tar: 0.859 | l_alg: 0.666 | l_hda: 0.076 | l_ttl: 0.753 | ent_tar: 0.144 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 12:51:56]\u001b[0m I:  5100/8000 | lr: 0.00022 | l_cls_src: 0.042 | l_cls_tar: 0.946 | l_alg: 0.676 | l_hda: 0.075 | l_ttl: 0.792 | ent_tar: 0.093 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 12:53:05]\u001b[0m I:  5150/8000 | lr: 0.00022 | l_cls_src: 0.005 | l_cls_tar: 1.629 | l_alg: 0.670 | l_hda: 0.076 | l_ttl: 0.751 | ent_tar: 0.153 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 12:54:23]\u001b[0m I:  5200/8000 | lr: 0.00022 | l_cls_src: 0.005 | l_cls_tar: 1.541 | l_alg: 0.668 | l_hda: 0.078 | l_ttl: 0.751 | ent_tar: 0.311 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 12:55:51]\u001b[0m I:  5250/8000 | lr: 0.00022 | l_cls_src: 0.006 | l_cls_tar: 0.886 | l_alg: 0.666 | l_hda: 0.072 | l_ttl: 0.744 | ent_tar: 0.182 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 12:57:16]\u001b[0m I:  5300/8000 | lr: 0.00022 | l_cls_src: 0.009 | l_cls_tar: 0.767 | l_alg: 0.685 | l_hda: 0.072 | l_ttl: 0.765 | ent_tar: 0.091 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 12:58:40]\u001b[0m I:  5350/8000 | lr: 0.00022 | l_cls_src: 0.004 | l_cls_tar: 1.340 | l_alg: 0.676 | l_hda: 0.072 | l_ttl: 0.753 | ent_tar: 0.107 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 13:00:03]\u001b[0m I:  5400/8000 | lr: 0.00022 | l_cls_src: 0.015 | l_cls_tar: 0.715 | l_alg: 0.638 | l_hda: 0.073 | l_ttl: 0.726 | ent_tar: 0.129 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 13:01:30]\u001b[0m I:  5450/8000 | lr: 0.00021 | l_cls_src: 0.010 | l_cls_tar: 0.843 | l_alg: 0.673 | l_hda: 0.072 | l_ttl: 0.754 | ent_tar: 0.162 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 13:02:57]\u001b[0m --> testing on source_test\n",
            "\u001b[32m[2022-07-25 13:03:26]\u001b[0m     I:  49/68 | acc: 99.603\n",
            "\u001b[32m[2022-07-25 13:03:30]\u001b[0m --> testing on target_test\n",
            "\u001b[32m[2022-07-25 13:04:30]\u001b[0m     I:  49/122 | acc: 81.576\n",
            "\u001b[32m[2022-07-25 13:05:21]\u001b[0m     I:  99/122 | acc: 81.229\n",
            "\u001b[32m[2022-07-25 13:05:36]\u001b[0m I:  5499/8000 | src_acc: 99.629 | tar_acc: 80.422 | best_acc: 80.445\n",
            "\u001b[32m[2022-07-25 13:05:36]\u001b[0m     models saved to exp/ckpt/art2real_world_123/models-last.pt\n",
            "\u001b[32m[2022-07-25 13:05:36]\u001b[0m     keys: dict_keys(['optimizer', 'ite', 'best_acc', 'base_net', 'd_net'])\n",
            "\u001b[32m[2022-07-25 13:05:39]\u001b[0m I:  5500/8000 | lr: 0.00021 | l_cls_src: 0.009 | l_cls_tar: 1.601 | l_alg: 0.723 | l_hda: 0.075 | l_ttl: 0.806 | ent_tar: 0.061 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 13:06:51]\u001b[0m I:  5550/8000 | lr: 0.00021 | l_cls_src: 0.085 | l_cls_tar: 1.370 | l_alg: 0.698 | l_hda: 0.071 | l_ttl: 0.855 | ent_tar: 0.176 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 13:08:21]\u001b[0m I:  5600/8000 | lr: 0.00021 | l_cls_src: 0.002 | l_cls_tar: 2.186 | l_alg: 0.667 | l_hda: 0.072 | l_ttl: 0.741 | ent_tar: 0.105 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 13:09:37]\u001b[0m I:  5650/8000 | lr: 0.00021 | l_cls_src: 0.029 | l_cls_tar: 1.254 | l_alg: 0.685 | l_hda: 0.071 | l_ttl: 0.785 | ent_tar: 0.182 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 13:11:13]\u001b[0m I:  5700/8000 | lr: 0.00021 | l_cls_src: 0.005 | l_cls_tar: 2.419 | l_alg: 0.705 | l_hda: 0.070 | l_ttl: 0.780 | ent_tar: 0.154 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 13:12:25]\u001b[0m I:  5750/8000 | lr: 0.00021 | l_cls_src: 0.002 | l_cls_tar: 1.647 | l_alg: 0.682 | l_hda: 0.070 | l_ttl: 0.754 | ent_tar: 0.170 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 13:13:43]\u001b[0m I:  5800/8000 | lr: 0.00021 | l_cls_src: 0.004 | l_cls_tar: 1.346 | l_alg: 0.685 | l_hda: 0.068 | l_ttl: 0.757 | ent_tar: 0.162 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 13:15:09]\u001b[0m I:  5850/8000 | lr: 0.00020 | l_cls_src: 0.002 | l_cls_tar: 0.998 | l_alg: 0.661 | l_hda: 0.069 | l_ttl: 0.732 | ent_tar: 0.086 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 13:16:32]\u001b[0m I:  5900/8000 | lr: 0.00020 | l_cls_src: 0.048 | l_cls_tar: 1.385 | l_alg: 0.673 | l_hda: 0.068 | l_ttl: 0.789 | ent_tar: 0.125 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 13:17:53]\u001b[0m I:  5950/8000 | lr: 0.00020 | l_cls_src: 0.013 | l_cls_tar: 0.604 | l_alg: 0.668 | l_hda: 0.070 | l_ttl: 0.751 | ent_tar: 0.057 | best_acc: 80.445 \n",
            "\u001b[32m[2022-07-25 13:19:08]\u001b[0m --> testing on source_test\n",
            "\u001b[32m[2022-07-25 13:19:40]\u001b[0m     I:  49/68 | acc: 99.603\n",
            "\u001b[32m[2022-07-25 13:19:44]\u001b[0m --> testing on target_test\n",
            "\u001b[32m[2022-07-25 13:20:46]\u001b[0m     I:  49/122 | acc: 81.519\n",
            "\u001b[32m[2022-07-25 13:21:36]\u001b[0m     I:  99/122 | acc: 81.397\n",
            "\u001b[32m[2022-07-25 13:21:51]\u001b[0m I:  5999/8000 | src_acc: 99.629 | tar_acc: 80.606 | best_acc: 80.606\n",
            "\u001b[32m[2022-07-25 13:21:52]\u001b[0m     models saved to exp/ckpt/art2real_world_123/models-last.pt\n",
            "\u001b[32m[2022-07-25 13:21:52]\u001b[0m     keys: dict_keys(['optimizer', 'ite', 'best_acc', 'base_net', 'd_net'])\n",
            "\u001b[32m[2022-07-25 13:21:52]\u001b[0m     best models saved to exp/ckpt/art2real_world_123/models-best.pt\n",
            "\u001b[32m[2022-07-25 13:21:55]\u001b[0m I:  6000/8000 | lr: 0.00020 | l_cls_src: 0.012 | l_cls_tar: 0.762 | l_alg: 0.690 | l_hda: 0.069 | l_ttl: 0.771 | ent_tar: 0.197 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:23:14]\u001b[0m I:  6050/8000 | lr: 0.00020 | l_cls_src: 0.003 | l_cls_tar: 1.205 | l_alg: 0.662 | l_hda: 0.068 | l_ttl: 0.733 | ent_tar: 0.128 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:24:37]\u001b[0m I:  6100/8000 | lr: 0.00020 | l_cls_src: 0.002 | l_cls_tar: 0.574 | l_alg: 0.663 | l_hda: 0.067 | l_ttl: 0.732 | ent_tar: 0.146 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:25:48]\u001b[0m I:  6150/8000 | lr: 0.00020 | l_cls_src: 0.002 | l_cls_tar: 1.184 | l_alg: 0.680 | l_hda: 0.064 | l_ttl: 0.746 | ent_tar: 0.052 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:27:17]\u001b[0m I:  6200/8000 | lr: 0.00020 | l_cls_src: 0.003 | l_cls_tar: 1.739 | l_alg: 0.676 | l_hda: 0.068 | l_ttl: 0.747 | ent_tar: 0.111 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:28:35]\u001b[0m I:  6250/8000 | lr: 0.00020 | l_cls_src: 0.005 | l_cls_tar: 1.705 | l_alg: 0.696 | l_hda: 0.069 | l_ttl: 0.769 | ent_tar: 0.133 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:30:05]\u001b[0m I:  6300/8000 | lr: 0.00019 | l_cls_src: 0.002 | l_cls_tar: 0.880 | l_alg: 0.656 | l_hda: 0.065 | l_ttl: 0.723 | ent_tar: 0.248 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:31:24]\u001b[0m I:  6350/8000 | lr: 0.00019 | l_cls_src: 0.003 | l_cls_tar: 0.936 | l_alg: 0.650 | l_hda: 0.066 | l_ttl: 0.720 | ent_tar: 0.137 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:32:42]\u001b[0m I:  6400/8000 | lr: 0.00019 | l_cls_src: 0.053 | l_cls_tar: 1.770 | l_alg: 0.640 | l_hda: 0.067 | l_ttl: 0.759 | ent_tar: 0.125 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:34:12]\u001b[0m I:  6450/8000 | lr: 0.00019 | l_cls_src: 0.026 | l_cls_tar: 2.254 | l_alg: 0.663 | l_hda: 0.065 | l_ttl: 0.754 | ent_tar: 0.283 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:35:20]\u001b[0m --> testing on source_test\n",
            "\u001b[32m[2022-07-25 13:35:49]\u001b[0m     I:  49/68 | acc: 99.603\n",
            "\u001b[32m[2022-07-25 13:35:53]\u001b[0m --> testing on target_test\n",
            "\u001b[32m[2022-07-25 13:36:55]\u001b[0m     I:  49/122 | acc: 81.349\n",
            "\u001b[32m[2022-07-25 13:37:45]\u001b[0m     I:  99/122 | acc: 80.948\n",
            "\u001b[32m[2022-07-25 13:38:01]\u001b[0m I:  6499/8000 | src_acc: 99.629 | tar_acc: 80.285 | best_acc: 80.606\n",
            "\u001b[32m[2022-07-25 13:38:01]\u001b[0m     models saved to exp/ckpt/art2real_world_123/models-last.pt\n",
            "\u001b[32m[2022-07-25 13:38:01]\u001b[0m     keys: dict_keys(['optimizer', 'ite', 'best_acc', 'base_net', 'd_net'])\n",
            "\u001b[32m[2022-07-25 13:38:08]\u001b[0m I:  6500/8000 | lr: 0.00019 | l_cls_src: 0.020 | l_cls_tar: 2.018 | l_alg: 0.652 | l_hda: 0.063 | l_ttl: 0.735 | ent_tar: 0.162 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:39:30]\u001b[0m I:  6550/8000 | lr: 0.00019 | l_cls_src: 0.011 | l_cls_tar: 1.432 | l_alg: 0.679 | l_hda: 0.065 | l_ttl: 0.754 | ent_tar: 0.151 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:40:46]\u001b[0m I:  6600/8000 | lr: 0.00019 | l_cls_src: 0.004 | l_cls_tar: 1.365 | l_alg: 0.689 | l_hda: 0.067 | l_ttl: 0.760 | ent_tar: 0.050 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:42:06]\u001b[0m I:  6650/8000 | lr: 0.00019 | l_cls_src: 0.010 | l_cls_tar: 1.190 | l_alg: 0.725 | l_hda: 0.064 | l_ttl: 0.799 | ent_tar: 0.076 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:43:37]\u001b[0m I:  6700/8000 | lr: 0.00019 | l_cls_src: 0.024 | l_cls_tar: 1.296 | l_alg: 0.719 | l_hda: 0.063 | l_ttl: 0.806 | ent_tar: 0.036 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:44:51]\u001b[0m I:  6750/8000 | lr: 0.00019 | l_cls_src: 0.006 | l_cls_tar: 1.686 | l_alg: 0.688 | l_hda: 0.062 | l_ttl: 0.756 | ent_tar: 0.130 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:46:21]\u001b[0m I:  6800/8000 | lr: 0.00018 | l_cls_src: 0.033 | l_cls_tar: 1.389 | l_alg: 0.671 | l_hda: 0.063 | l_ttl: 0.766 | ent_tar: 0.213 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:47:40]\u001b[0m I:  6850/8000 | lr: 0.00018 | l_cls_src: 0.009 | l_cls_tar: 0.885 | l_alg: 0.693 | l_hda: 0.063 | l_ttl: 0.764 | ent_tar: 0.062 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:49:01]\u001b[0m I:  6900/8000 | lr: 0.00018 | l_cls_src: 0.012 | l_cls_tar: 0.845 | l_alg: 0.716 | l_hda: 0.062 | l_ttl: 0.790 | ent_tar: 0.090 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:50:29]\u001b[0m I:  6950/8000 | lr: 0.00018 | l_cls_src: 0.054 | l_cls_tar: 1.030 | l_alg: 0.730 | l_hda: 0.062 | l_ttl: 0.845 | ent_tar: 0.126 | best_acc: 80.606 \n",
            "\u001b[32m[2022-07-25 13:51:44]\u001b[0m --> testing on source_test\n",
            "\u001b[32m[2022-07-25 13:52:15]\u001b[0m     I:  49/68 | acc: 99.660\n",
            "\u001b[32m[2022-07-25 13:52:19]\u001b[0m --> testing on target_test\n",
            "\u001b[32m[2022-07-25 13:53:20]\u001b[0m     I:  49/122 | acc: 81.406\n",
            "\u001b[32m[2022-07-25 13:54:11]\u001b[0m     I:  99/122 | acc: 81.145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "import argparse\n",
        "from argparse import Namespace\n",
        "\n",
        "import torch.backends.cudnn as cudnn\n",
        "import easydict\n",
        "\n",
        "from configs.defaults import get_default_and_update_cfg\n",
        "from utils.utils import create_logger, set_seed\n",
        "from trainer import *\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    \n",
        "    '''parser = argparse.ArgumentParser()\n",
        "    \n",
        "    parser.add_argument('cfg',      default='configs/test.yaml', type=str)\n",
        "    parser.add_argument('--seed',   default=123, type=int)\n",
        "    parser.add_argument('--source', default='a', nargs='+', help='source domain names')\n",
        "    parser.add_argument('--target', default='r', nargs='+', help='target domain names')\n",
        "    parser.add_argument('--output_root', default=\"exp\", type=str, help='output root path')\n",
        "    parser.add_argument('--output_dir',  default='op', type=str, help='output path, subdir under output_root')\n",
        "    parser.add_argument('--data_root',   default='data/office_home', type=str, help='path to dataset root')\n",
        "    parser.add_argument('--opts',   default=None, nargs=argparse.REMAINDER)'''\n",
        "    args = easydict.EasyDict({\n",
        "        \"cfg\": 'configs/test.yaml',\n",
        "        \"seed\": 123,\n",
        "        \"source\": 'r',\n",
        "        \"target\": 'a',\n",
        "        \"output_dir\": 'od',\n",
        "        \"outpuy_root\": 'exp',\n",
        "        \"data_root\": 'data/office_home'\n",
        "})\n",
        "    #args = parser.parse_args()\n",
        "    #args, unknown = parser.parse_known_args()\n",
        "    assert os.path.isfile(args.cfg), 'cfg file: {} not found'.format(args.cfg)\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    cfg = get_default_and_update_cfg(args)\n",
        "    cfg.freeze()\n",
        "\n",
        "    # seed\n",
        "    set_seed(cfg.SEED)\n",
        "\n",
        "    cudnn.deterministic = True\n",
        "    cudnn.benchmark = False\n",
        "\n",
        "    # logger\n",
        "    logger = create_logger(cfg.TRAIN.OUTPUT_LOG)\n",
        "\n",
        "    logger.info('======================= args =======================\\n' + json.dumps(vars(args), indent=4))\n",
        "    logger.info('======================= cfg =======================\\n' + cfg.dump(indent=4))\n",
        "\n",
        "    trainer = eval(cfg.TRAINER)(cfg)\n",
        "    trainer.train()\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "qpcjmn3jbEqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9dtxpew-xpj3"
      }
    }
  ]
}